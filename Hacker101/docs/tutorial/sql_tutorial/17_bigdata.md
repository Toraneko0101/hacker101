# クイズ(17:BigData)

## Q151 ビッグデータという概念について知っていますか?

??? success
    ### ビッグデータ概要(3つのV)

    ```text
    Volume(データの量)
      ・数十億や数兆単位のデータがあるか
    
    Velocity(データの速度)
      ・データがどのくらい素早く届くか
      ・速いというのは、データが更新される頻度
      （常に変化し続けるデータで、現在に即した結果を導く）
    
    Variety(データの多様性)
      ・データが常に構造化されているとは限らず、
        非構造化データとして提供される
    
    これを踏まえてビッグデータとは
      ・従来のシステムでは処理できないほど巨大なデータ
      ・従来の構造に収まらない複雑なデータ
      ・常に変化し続けるデータ


    最近はあと2つのVもある
      Veracity(正確性)
      Value（価値）⇒データを組み合わせて新たな価値を生み出す
    
    ```

    ###　ビッグデータの種類

    ```text
    構造化データ
      ・従来のRDBに格納できるデータ
    
    準構造化データ
      ・ログやJSONなどの完全な構造定義を持たないデータ
    
    非構造化データ
      ・文書、音声、動画、画像など、データ部に構造定義を
        まったくもたないデータ
    ```

    ### なぜビッグデータが登場してきたのか

    ```text
    [技術の発展]
      ・分散処理技術(Hadoop)やクラウドサービスの充実
      ・NoSQL-DB, 機械学習などの発展
    
    [データの増加]
      ・IoT家電の登場や、SNS、Webシステムなどにより
        生成されるデータが以前より増えた
    
    [低価格化]
      ・データを取得し、送信するセンサーなどの低価格化
      ・クラウドなどもここに含まれる
    ```

    ### ビッグデータの格納場所

    ```text
    [自社]
      ・自社の基幹システム
      ・Web,SNSサービスなどの操作ログ等
    
    [社外]
      ・スマホ/家電の使用量などのデータ
      ・企業間で共有されるデータ
    
    [一般]
      ・政府や自治体などで公開されたデータ
      ・データ提供事業者から入手したデータ
    ```

    ### オープンデータ

    ```text
    [特徴]
    ・誰でも入手可能で、利用/再配布も可能なデータ
    ・コンピュータから利用可能
    ・特許/著作権周りの問題もない

    ⇒上記の条件に当てはまるならオープンデータ

    [公開している機関]
    ・政府、地方自治体、研究機関、大学、民間企業

    [オープンデータの例]
    ・国勢調査データ
    ・気象データ

    [使用例]
    ・気象データと、自社の販売データの相関関係を考える
    ・多変量解析周りの、統計的な知識が必要。
    ・因子分析等の知識がないと結果を違える

    [結果を違える例]
    ・1950年頃の過去データを用い、採用活動に用いる
    ⇒人種間や性別間の偏見が残ってしまう

    ```

    ### 各分野におけるビッグデータ

    ```text
    マーケティング
      ・Webのログ(準構造データ)やカメラ(非構造データ)
        から顧客の行動を分析
    
    製品開発
      ・センサ（準構造データ）や顧客の声（非構造データ）
        をもとに、開発の方針を決定
    
    コンプライアンス
      ・正しい文章と、誤った文章(非構造データ)を元に
        学習させ、コサイン類似度などで、不適切な文言をはじく

    セキュリティ
      ・サイバー攻撃のパターンをログ(準構造データ)
      　などから検知して、予防する

    メンテナンス
      ・センサデータの故障予測を行う
    
    社会インフラ
      ・過去のデータをもとに、災害の予測を立てる
    ```

    ### BigDataを使ったシステムについて具体案を出してみる

    ??? info

        ```text

        ・実際に病院で診断を下された患者(考えすぎも含む)が、
          Web上でどのような検索ワードを用いて
          自身の病状を検索していたのか
          (もちろん、個人を識別できるような情報は削除する)

        -----------------------

        ・現在のWebにおいて、医療関係の情報は
          人を不安にさせるものが多い
        ・たとえば、鼻水が出ると調べれば、
          脳みそがジュースになっているという検索結果が出るし、
          元気がないと調べれば、筋ジスだと返ってくる

        (ちなみに私は、スギ花粉による花粉症に
          毎年春先苦しめられているが、
          以前「AI診断」と検索して出てきたサービスに
          自身の症状を入力したところ、
          「今すぐ病院に行くべきで、難病にかかっている
          可能性がある」と言われた。控え目に言って糞だ)
        
        ・この結果、患者と医師はどうなるか

        ・患者の場合、不安に駆られる。
          年がら年中身体のことを考えることで、
          病院に行く回数は増えるだろうが、
          これは健康とは言えないだろう。精神的に病んでいる

        ・また、現場の医師は明らかに不利益を被る
        　ネットで調べ、不安になった人間が病院を訪れ
          自身は重い病気なんだと主張するからだ
          
        ・そして、この手の患者は
          医師に大丈夫ですと言われても安心しない。
          仮に一時心の平穏を取り戻せたとしても、
          数日後には別の箇所が心配になり、また調べ、
          そして不安になり、医師に相談することになる。
          この繰り返しだ。これが死ぬまで続く。
        
        ・つまるところ、ネットにあふれる健康情報は
        　百害あって一利ない。

        ・そして、一利ないということを患者に分からせるためには
          実際のデータを持ってくるのが一番である

        ・「鼻水　水っぽい　一日中」と調べた患者のうち、
          何%が、脳みそがジュースになっている病気だったのか。
          そして、何%が、花粉症だったのか。
        
        ・小さなしこりがあると調べた患者のうち、
          何％が悪性腫瘍で、何%が脂肪種だったのか。
        
        ・是非、問診表の中で、
          ここに来るまでにネットで病気に関するワードを
          打ち込んだか記載させ、
          実際の病状と照らし合わせてほしい。
          患者の心の平穏にも、現場の医師のためにもなるはずだ。

        ※これは出所のはっきりしている情報
        （たとえば医師が書いているサイト）にも言える話である

          大体、責任問題になるのだから、医師が絶対に大丈夫
          というわけがない。「一度診断を受けるべき」
          気軽な気持ちで、こう締めくくるわけだ。
          で、心気症に近い患者は、違う部位が気になるたびに
          「一度診断を受けるべき」と思い、病院を訪れ
          病院はパンクする。私はこの悪循環をなくしたい

        ```
    
## Q152 個人情報とビッグデータの関係性について知っていますか?

??? success
    ### データの集め方について

    ```text
    ・個人情報保護法等を守る必要がある

    ・また特定の国に限らず、情報収集をする場合は、
      該当国・地域の法律も順守する(GDPRとかね)
    ```

    ### 個人情報とビッグデータ

    ```text
    個人情報
      ・個人(生存者のみ)の氏名、生年月日、その他の記述により
        特定の個人を識別できるような情報
    
    個人情報取扱事業者
      ・個人情報をDB等で所持し、事業に用いている事業者
      ・取扱件数にかかわらない(2015年から)
      ・流失の際、報告や改善措置を怠ると刑事罰の対象になる
    
    個人情報取得の際には
      ・オプトイン
      　⇒（利用規約などで、予め本人に利用目的を明示して
            同意のチェックを付けさせる）

      ・オプトアウト
        ⇒（拒否しない限り、同意とみなす
            Webサイトのクッキーなどに多い）

      ・情報を第三者に提供する場合については、
      　通常予め同意を取っておく必要がある(オプトイン)
      
      ・オプトアウト方式で第三者に情報提供する場合、
        プライバシーポリシー等に必要事項を記載し、
        個人情報保護委員会にその旨を申し立てる必要がある

      ・2022年からは開示請求で、第三者にどのように提供された
      　のか確かめられるようになった
      
    
    要配慮情報について
      ・人種、信条、病歴などの要配慮情報は
        オプトアウト方式では第三者に提供できない
      
    ```

## Q153 匿名加工処理について知っていますか?

??? success
    ### 摂動法とは

    ```text
    ・データの加工や変換において、
      元のデータを特定の方法で変更し、
      元に戻す事が出来ないようにする技術
    
    ・プライバシー保護やデータの安全性の観点から行われる

    ・暗号法と異なり、解読することができないので
      正しく処理すれば復元することは困難になる

    ※そもそも摂動とは
      ・力学の言葉で、乱すこと
      ・セキュリティでいうと、ランダムノイズ等を書けて
        元のデータを分からなくするように乱している
    ```

    ### 前提（識別子/準識別子）

    ```text
    識別子
      ・個人をはっきり指し示す属性（マイナンバーとか）
    
    準識別子
      ・年齢、性別、居住地など組み合わせることで
        個人の特定が可能になる属性
      
    要配慮情報
      ・センシティブ属性とも呼ぶ
      ・人に知られたくないことは全部そう
      ・年収とかも
    
    ⇒要配慮情報と個人を結び付けられないよう匿名化を行う
    ```

## Q154 K-匿名化について知っていますか?

??? success
    ### K-匿名化(摂動法)

    ```text
    ・同じような人が、必ずK人以上いる状態を作ること
      （準識別子が全く同一の個人が少なくともK人存在）

    [抑制]
      ・特定の属性の値を*で変換
      ・識別子は必ず変換
      ・必要であれば準識別子も
    
    [一般化]
      ・97歳を90代などに変換
      ・個々の属性値を広い範囲に置換する
    
    1 抑制、一般化を行ったうえで、準識別子の組み合わせを
      調べる
    
    2 どの組み合わせを用いても、K個以上のレコードが
      残る場合、K-匿名性を満たすという
    
    [弱点：一般的に]
      ・ランダム性を含まないため、推測が可能

    [弱点:背景知識攻撃]
      ・攻撃者が何らかの背景知識を持っている場合、
      （たとえば要配慮情報の中身を大まかに知っている場合）
      　明らかに当てはまらないものを除くことで、
      　個人が特定される可能性がある
    
    [弱点：同種攻撃]
      ・要配慮情報を含めたすべての機密属性が同じ場合、
        レコードを特定できなくとも、機密情報がばれる
    ```

    ```sql
    -- 前処理前
    -- 宗教を要配慮情報とした場合

    select 
      "Nekotaro" name,
      38 age,
      1 sex,
      "Tokyo" region,
      "Nekozyara-Kyo" belief
    union all
    select 
      "Torakiti" name,
      32 age,
      1 sex,
      "Tokyo" region,
      "Nekozyara-kyo" belief
    union all
    select 
      "LionMaru" name,
      59 age,
      1 sex,
      "Osaka" region,
      "Tategami-Kyo" belief
    union all
    select 
      "Kabanosuke" name,
      52 age,
      1 sex,
      "Osaka" region,
      "Mizuabi-Club" belief
    ;

    /*
    +------------+-----+-----+--------+---------------+
    | name       | age | sex | region | belief        |
    +------------+-----+-----+--------+---------------+
    | Nekotaro   |  38 |   1 | Tokyo  | Nekozyara-Kyo |
    | Torakiti   |  32 |   1 | Tokyo  | Nekozyara-kyo |
    | LionMaru   |  59 |   1 | Osaka  | Tategami-Kyo  |
    | Kabanosuke |  52 |   1 | Osaka  | Mizuabi-Club  |
    +------------+-----+-----+--------+---------------+
    4 rows in set (0.000 sec)
    */

    -- K-多様性適用後
    select 
      "*" name,
      "30s-50s" age,
      1 sex,
      "Japan" region,
      "Nekozyara-Kyo" belief
    union all
    select 
      "*" name,
      "30s-50s" age,
      1 sex,
      "Japan" region,
      "Nekozyara-kyo" belief
    union all
    select 
      "*" name,
      "30s-50s" age,
      1 sex,
      "Japan" region,
      "Tategami-Kyo" belief
    union all
    select 
      "*" name,
      "30s-50s" age,
      1 sex,
      "Japan" region,
      "Mizuabi-Club" belief
    ;

    /*
    準識別子をage,sex,regionとすると、
    2-匿名性が担保されているが......

    30代と判明した場合、同種攻撃で突破されることが分かる
    */

    /*
    +------+-----+-----+--------+---------------+
    | name | age | sex | region | belief        |
    +------+-----+-----+--------+---------------+
    | *    | 30s |   1 | Tokyo  | Nekozyara-Kyo |
    | *    | 30s |   1 | Tokyo  | Nekozyara-kyo |
    | *    | 50s |   1 | Osaka  | Tategami-Kyo  |
    | *    | 50s |   1 | Osaka  | Mizuabi-Club  |
    +------+-----+-----+--------+---------------+
    4 rows in set (0.001 sec)
    */
    ```

## Q155 L-多様性について知っていますか?

??? success

    ### L-多様性(摂動法)

    ```text
    ・漏えいさせたくない属性が、
      同一グループ内で、
      少なくとも「L種類以上ある」状態を作ること
    
    ※同一グループ：準識別子が全て同じ集まり

    ⇒L=2以上の場合、同値攻撃も不可
    ⇒準識別子を照らし合わせても、個人の情報を特定できなくなる
    
    [Lを増やすには]
      ・一般化の手法を見直して、少数のグループをなくす

      ・たとえば以下の例の場合、ageとregionの幅を
        大きくすることで、Lを増加させている
      
      ・どうしてもLが広げられない場合は、
        そこだけ*で匿名化する方法もある
    
    ・以下の場合、L = 1である
      +------+-----+-----+--------+---------------+
      | name | age | sex | region | belief        |
      +------+-----+-----+--------+---------------+
      | *    | 30s |   1 | Tokyo  | Nekozyara-Kyo |
      | *    | 30s |   1 | Tokyo  | Nekozyara-kyo |
      | *    | 50s |   1 | Osaka  | Tategami-Kyo  |
      | *    | 50s |   1 | Osaka  | Mizuabi-Club  |
      +------+-----+-----+--------+---------------+
      4 rows in set (0.001 sec)
    
    ・これはL = 3
    +------+---------+-----+--------+---------------+
    | name | age     | sex | region | belief        |
    +------+---------+-----+--------+---------------+
    | *    | 30s-50s |   1 | Japan  | Nekozyara-Kyo |
    | *    | 30s-50s |   1 | Japan  | Nekozyara-kyo |
    | *    | 30s-50s |   1 | Japan  | Tategami-Kyo  |
    | *    | 30s-50s |   1 | Japan  | Mizuabi-Club  |
    +------+---------+-----+--------+---------------+
    4 rows in set (0.001 sec)  
  
    ```

## Q156 T-近接性について知っていますか?

??? success

    ### T-近接性(摂動法)

    ```text
    [概要]
    ・任意の準識別子グループ内の要配慮情報の分布と、
      全体の要配慮情報との分布の差がt以下である
    
    [k-匿名化やL-多様性との違い]
      ・これら二つは個数の下限を考えていた
      ・t-近接性は、差の上限を保証する
    
    [t-近接性を満たさない例]
      ・データセット全体の場合、収入の分布は散らばっている
      ・特定のグループには、特定範囲の収入情報しか入っていない
    ⇒攻撃者は特定の準識別子を持った人の傾向を
      知ることができてしまう

    [t-近接性を満たす例]
      ・データセット全体の場合、収入の分布は散らばっている
      ・特定のグループを見ても、収入の分布は変わらない
      ⇒攻撃者は準識別子を頼りに探しても、
        全体の分布以上の特徴を知ることができない
    
    [t-近接性のデメリット]
      ・データ利用者の得られる情報も制限してしまうので
        データを価値あるものとして扱うことが難しくなる
    ```

## Q157 差分プライバシについて知っていますか?

??? success

    ### 差分プライバシ(摂動法)

    ```text
    [概要]
      ・データ全体ではなく、クエリに対して加工を行う

      ・どれだけ他人と見分けがつかないかを、ε(プライバシー強度)
        で表現する
    
    [モザイク効果/再構築攻撃への対策]
        ・モザイク効果は、1つでは安全そうに思えたデータセットが
          データセットを複数重ね合わせたとき、
          プライバシーの暴露を引き起こす事象を指す
        
        ・また、複数のデータを重ね合わせて、
          個人の情報を特定する攻撃を、再構築攻撃という
        
        ・ここでk-匿名化などの手法が、単一のデータセットに
          終始していたことを思い出す。
        
        ・つまり、k-匿名化などの手法は、一見安全そうに見えても
          再構築攻撃による潜在的な危険に対応できない
        
        ・差分プライバシは、統計的な尺度を導入することで、
          これらの問題点や潜在的な危険に対応する
      
    [差分プライバシの考え方]
      ・プライバシー保護を施した特定のqueryに対し、
        どれくらいプライバシーが損なわれるかをε(>=0)で表現する
      
      ・値が大きいほど、プライバシーが漏洩する

      ・特定のqueryのプライバシー損失がε以下の場合、
        queryは、ε-差分プライバシー(ε-DP)を満たすという
      
      ・具体的には、以下2つのデータベースについて考える
      　D1：Aさんのデータが含まれるデータベース
        D2: Aさんのデータを他の誰かの物に置き換えたデータベース
        (一行だけ異なる2つのDBを隣接データベースという)
        (※なお、置き換えるのではなく、除くこともある)
      
      ・単純にクエリを適用すると、その差分から
        Aさんの個人情報が導き出されてしまうため
      　何らかの確率分布をもとにランダム化関数を適用し
      　結果クエリを算出する
      

      ・その結果、見分けがつかなければ、D1とD2の結果クエリから
      　Aさんの情報を入手することができないといえ、
        ε-DPは小さくなる。逆もまた然り
      
    [何らかの確率分布?]
      ・Laplace分布が有名
      ・分散が大きくなれば、それだけ特定しにくくなるため
        εは小さくなる
    
    [デメリット]
      ・プライバシーの消費は累積し、続ければ効果がなくなる
      ・分散が大きくなるということは、精度が下がる事を意味する
    
    [プライバシ以外のメリット]
      ・データそのものを抑制したり、一般化を行うk-匿名化と比べ
      　差分プライバシは個々のクエリに誤差を加えただけなので、
        統計的な傾向は守られる

        ⇒統計的分析に使用できる
      
      
    ```

## Q158 準同型暗号方式について知っていますか?

??? success

    ### 準同型暗号方式(暗号法)

    ```text
    [秘密計算]
      ・データを暗号化したまま計算できる技術を指す
      ・機密データを送受信する過程で、
        復号、暗号化を繰り返す必要がなくなる
    
    [準同型暗号方式]
      1 公開鍵を利用し、複雑な暗号関数を作る
      2 暗号化した状態でデータの計算を委託(計算鍵も同封)
      3 委託先の企業は暗号化した状態で計算(計算鍵を利用)
      4 暗号化した結果を委託元に戻す
      5 委託元の企業は、復号用の秘密鍵を持っているため、
        それを利用してデータの内容が判定できる
    
    [準同型暗号方式のデメリット]
      ・暗号化したデータが巨大になり、コストがかかる
      ・鍵が流出すると終わる
      ・鍵をなくすと、復号できなくなる
    ```

## Q159 秘密分散方式について知っていますか?

??? success

    ### 秘密分散方式(暗号化)

    ```text
    [秘密分散方式]
      秘匿情報を分割して、保管しておくこと
    
    [(k,n)しきい値法 -> 秘密分散方式の一つ]
      1 データをn個の断片（シェア）に分割する
      2 n個のうち、k個以上が集まれば復元可能
      2 シェアは、複数のサーバに1つずつ分散させる
      3 委託先は、シャアのうちk個未満の情報を集め
        処理を行い、結果を出力
      4 委託元は、分散した結果をすべて集め、k個以上
        集めることで復元する
    
    [メリット]
      ・k-1個のサーバまでなら、侵入されても
      　何の情報も得られないため機密性において軍配があがる

      ・n-k個までなら故障しても、残りのコンピュータから
        データを復元可能

    [分割手法について(一例)]
      ・k-1次多項式の曲線を特定するには、
      　k個のデータ点があればいい。

      ・これを利用して、k個のシェアから、k-1次多項式を解き、
        秘密情報である切片を入手する
      ⇒シャミアの秘密分散法
    ```

## Q160 ビッグデータとクラウドの関係性について知っていますか?

??? success
    ### クラウドサービスの種類

    ```text
    SaaS(Software as a Service)
      ・インターネットを経由して、ソフトウェアパッケージを提供
        するサービス
    
    PaaS(Platform as a Service)
      ・インターネットを経由して、アプリの開発・運用環境全体
        を提供するサービス
    
    IaaS(Infrastructure as a Service)
      ・昔はHaaS(Hardware as a Service)と呼ばれていたことも
      ・インターネントを経由して、ハードウェアや回線などの
        インフラを提供するサービス
      ・ユーザはハードウェアを物理的に所有せずに済む
    ```

    ### クラウドサービスで大規模データを扱う利点

    ```text
    コスト面
      ・先行投資（ハードウェアの準備等）が不要

      ・自動でスケーリングを行い、繁忙期や時間帯に合わせて、
        処理能力を変更でき、コスト削減できる

    -------------


    安全性
      ・複数のAZやリージョンに、データを複製しているため
        耐障害性という面で優れている
    
    -------------


    運用面
      ・ビッグデータは基本的にデータが日増しになっていく
        クラウドなら、容量拡大に伴う作業が不要
    
    -------------

    人的資源
      ・インフラの構築に割いていた時間を、
        本当に必要な業務に回すことができる

    -------------

    独自のカスタマイズができない
      ・企業独自のカスタマイズをさせられずに済むのは
        転職が多い業界にとってはむしろメリットだろう

      ・下手に必要以上のカスタマイズができてしまうと、
        却って本来の解決策が見つけにくくなるかもしれない
    ```

    ### 欠点

    ```text
    世界規模の障害が起こった場合
      ・クラウドサービスで障害が起こると、
        業務に影響が出る場合がある
    ```

## Q161 ビッグデータとIoTの関係性について知っていますか?

??? success

    ### 基本

    ```text
    「モノ」から取得したデータをビッグデータの一部として処理し、
    統計的分析や、機械学習で用いる
    ```

    ### IoT関連ワード概要

    ```text
    Iot(Internet of Things)
      ・モノがインターネットに接続できるようになった状態

      ・20世紀まではインターネットとは関係のなかったもの
        たとえば家電や、時計、眼鏡、自動車などが
        ネットワークにつながり、相互に情報交換が
        可能になった状態を意味する
      
      ・言葉ができた90年代当初は
        単にRFID(radio frequency identification)のことを
        指していたが、今は上の定義が正しい

    -------------

    ユビキタス社会という考え方
      ・そもそもubiquitousがいたるところにという意味
      ・つまりいつでもどこでもinternetを利用できる社会を指す
      ・IoTとの違いは人視点であること
    
    --------------

    M2Mという言葉について
      ・Machine to Machine
      ・機械同士が、人を介在しないで情報をやり取りするシステム
      ・必ずしもインターネントを介す必要はない
    
    ⇒つまり、IoTはユビキタス社会とM2Mを包括する
    ```

    ### 取得したデータを用いてデータ分析するまでの流れ

    ```text
    ※用語については下で補完する

    1 デバイスに埋め込んだセンサを用いて、データを取得する

    2 デバイスから受信サーバへとデータ送信
    
    3 処理サーバがストリーム方式やバッチ処理で
      データを加工する。
    ⇒リアルタイムで情報が必要なら、デバイスに送り返す
    
    4 加工済みのデータを、蓄積サーバで保管

    6 必要に応じ、統計的手法や、機械学習を用いて分析を行う
    ```

    ### 上記の用語補足

    ```text
    センサ
      ・物理的な現象を検知して、電気信号として出力する装置
    
    デバイス
      ・センサを介して、ネットワークに接続された装置、モノ

    受信サーバ
      ・HTTP/MQTT/WebSocket等の通信方法を用い、
      　デバイスとのデータの送受信を行う

    処理サーバ
      ・ETLツールなどを用い、クレンジングを行う
    
    クレンジング
      ・データの整理や加工を行うこと
    
    蓄積サーバ
      ・ビッグデータを保管するサーバ
    ```

    ### なぜ受信/処理/蓄積サーバを1つにまとめないのか

    ```text
    ・各サーバが異なる役割を持つから

    受信サーバ
      ・データ転送速度等を最適化
    
    処理サーバ
      ・計算負荷に対応するために強力なCPUが必要
      ・大量のメモリもいる
    
    蓄積サーバ
      ・大量のデータを保管するために、大容量ストレージが必要
    
    各サーバを分けることで
      ・それぞれをスケールアップ/スケールアウト可能
      ・システムを柔軟に組むことができる
    ```

    ### センサ補足

    ```text
    概要
      ・一つのデバイスに対して、
        複数のセンサが埋め込まれる事が多い

    
    画像センサ
      ・画像や動画を作成
      ・赤外線を検知して、画像処理するものもある
      ・工場の欠けている部品をチェックしたり

    光センサ
      ・光の強度を測定する
      ・スマホで言うと、画面の明るさの自動調整機能
    
    温度/湿度センサ
      ・温度, 湿度を測定する
      ・エアコンの自動調整等に使う
    
    振動/速度/加速度センサ
      ・機器の振動, 速度, 加速度の測定
      ・自動車などには不可欠だろう
    
    地磁気センサ
      ・地磁気を検出することで方角を計測
      ・カーナビやGPS端末の電子コンパスとしても使用される
    
    ジャイロセンサ
      ・デバイスの傾きを検知する
      ・昔のカービィのゲームであったな
    
    音声マイク
      ・機器が発する音や、人の声などの音声を収集する
      ・カメラの録音機能や、工場の異音検知等はこれ
    
    ```

    ### デバイス補足

    ```text
    デバイスの機能
      ・センシング
      ・フィードバック
    
    センシング
      ・センサーを利用して、デバイス本体や周囲環境を収集し、
        IoTシステムに通知すること
      
    フィードバック
      ・システムからの通知を受け、指示や動作を
        機器に返すこと
    ```

    ### センサが取得するデータのフォーマット

    ```text
    XML
      ・人が呼んで分かりやすい形式
      ・またはデータ量が多い場合に使用
      ⇒SQLでの使用法については次の節で行う
    
    JSON
      ・データ量が少ない場合に使用
      ⇒SQLでの使用法については次の節で扱う
    
    MessagePack
      ・バイナリデータをそのまま扱いたい場合に使用
      ・パースする必要がない
      ・形式自体はJSONに似ている
      ・シリアライズ、デシリアライズともに非常に高速
      ・データサイズは小さく、ストリーム処理が可能な特徴を持つ
    ```
## Q162 XMLをSQLで扱う方法を知っていますか?

??? success

    ### XMLファイルをSQLのテーブルとして扱う

    ```sql
    use sakila;

    create table person (
      person_id int not null primary key,
      fname varchar(40) null,
      lname varchar(40) null,
      created timestamp
    );
    ```

    ```xml
    <!-- test.xml -->
    <!-- 仮にtableに対応するカラムがない場合は、skipされる-->
    <!-- fieldの指定順序は関係ない -->
    <list>
      <person person_id="1" fname="Neko" lname="Taro" />
      <person person_id="2" fname="Inu" lname="Ziro" />
      <person person_id="3">
        <fname>Tora</fname>
        <lname>Kuroo</lname>
      </person>
      <person>
        <field name="person_id">4</field>
        <field name="fname">Onigawara</field>
        <field name="lname">Momizi</field>
      </person>
    </list>
    ```

    ```sql
    -- resultsetやrow等の目印がないため
    -- どこからがrowなのかを示すため3行目が必要
    
    /*
    local
      ・MySQLサーバは一時ファイルを格納するディレクトリに
        ファイルのコピーを作成する
      ・これがない場合、ファイルはサーバホストからしか読めない
    */
    load xml local infile "test.xml"
      into table person
      rows identified by "<person>"
    ;

    select * from person;

    /*
    +-----------+-----------+--------+---------------------+
    | person_id | fname     | lname  | created             |
    +-----------+-----------+--------+---------------------+
    |         1 | Neko      | Taro   | 2024-04-13 16:43:31 |
    |         2 | Inu       | Ziro   | 2024-04-13 16:43:31 |
    |         3 | Tora      | Kuroo  | 2024-04-13 16:43:31 |
    |         4 | Onigawara | Momizi | 2024-04-13 16:43:31 |
    +-----------+-----------+--------+---------------------+
    4 rows in set (0.001 sec)
    */
    ```

    ### SQLのテーブルをXMLに変換する

    ```bash
    mysql -unekoinu -pnyanko  \
    --xml -e "select * from sakila.person" \
    > person-dump.xml

    cat person-dump.xml
    ```

    ```xml
    <!-- person-dump.xml -->

    <?xml version="1.0"?>

    <resultset statement="select * from sakila.person
    " xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
      <row>
            <field name="person_id">1</field>
            <field name="fname">Neko</field>
            <field name="lname">Taro</field>
            <field name="created">2024-04-13 16:43:31</field>        
      </row>

      <row>
            <field name="person_id">2</field>
            <field name="fname">Inu</field>
            <field name="lname">Ziro</field>
            <field name="created">2024-04-13 16:43:31</field>        
      </row>

      <row>
            <field name="person_id">3</field>
            <field name="fname">Tora</field>
            <field name="lname">Kuroo</field>
            <field name="created">2024-04-13 16:43:31</field>        
      </row>

      <row>
            <field name="person_id">4</field>
            <field name="fname">Onigawara</field>
            <field name="lname">Momizi</field>
            <field name="created">2024-04-13 16:43:31</field>        
      </row>
    </resultset>
    ```

    ```sql
    --　出力結果をさらにtableに
    create table person2 like person;

    --resultsetやrowがあるので、row句は短縮できる
    load xml local infile "person-dump.xml"
    into table person2;

    /*
    +-----------+-----------+--------+---------------------+
    | person_id | fname     | lname  | created             |
    +-----------+-----------+--------+---------------------+
    |         1 | Neko      | Taro   | 2024-04-13 16:43:31 |
    |         2 | Inu       | Ziro   | 2024-04-13 16:43:31 |
    |         3 | Tora      | Kuroo  | 2024-04-13 16:43:31 |
    |         4 | Onigawara | Momizi | 2024-04-13 16:43:31 |
    +-----------+-----------+--------+---------------------+
    4 rows in set (0.001 sec)
    */
    ```

    ### フィールド名を、カラム名と結び付けたい場合

    ```sql
    -- 同じフィールドを使うのは汎用性が低い

    create table individual (
      individual_id int not null primary key,
      name1 varchar(40) null,
      name2 varchar(40) null,
      made timestamp
    );

    -- fieldの値をユーザ変数とし、setを利用して、格納
    load xml local infile "./person-dump.xml"
      into table individual (
        @person_id, @fname, @lname, @created
      )
      set 
        individual_id = @person_id,
        name1 = @fname,
        name2 = @lname,
        made = @created
    ;

    select individual_id, name1, name2  from individual;

    /*
    +---------------+-----------+--------+
    | individual_id | name1     | name2  |
    +---------------+-----------+--------+
    |             1 | Neko      | Taro   |
    |             2 | Inu       | Ziro   |
    |             3 | Tora      | Kuroo  |
    |             4 | Onigawara | Momizi |
    +---------------+-----------+--------+
    4 rows in set (0.001 sec)
    */
    ```

## Q163 JSONをSQLで扱う方法を知っていますか?

??? success
    ### 行挿入

    ```sql
    /*型はJSON*/
    create table json_sample(
      id int, 
      item json
    );

    /*json_objectの形か、下記の様に挿入*/
    insert into json_sample values
    (1, '{"fruit" : ["apple", "peach"]}'),
    (2, '{"drink" : ["tea", "samovar", "Matcha"]}')
    ;

    insert into json_sample values
    (
      3, (
        JSON_OBJECT(
          'num', '[1,2,3,4]', 
          'double_num', '[2,4,6,8]'
        )
      )
    );

    select * from json_sample;
    /*
    +------+-------------------------------------------------+
    | id   | item                                            |
    +------+-------------------------------------------------+   
    |    1 | {"fruit" : ["apple", "peach"]}                  |   
    |    2 | {"drink" : ["tea", "samovar", "Matcha"]}        |   
    |    3 | {"num": "[1,2,3,4]", "double_num": "[2,4,6,8]"} |   
    +------+-------------------------------------------------+   
    3 rows in set (0.001 sec)
    */
    ```

    ### 集約

    ```sql
    select group_concat(item) from json_sample;
    /*
    +-------------------------------------------------------------------------------------------------------------------------+
    | group_concat(item)                                                                                                      |
    +-------------------------------------------------------------------------------------------------------------------------+
    | {"fruit" : ["apple", "peach"]},{"drink" : ["tea", "samovar", "Matcha"]},{"num": "[1,2,3,4]", "double_num": "[2,4,6,8]"} |
    +-------------------------------------------------------------------------------------------------------------------------+
    1 row in set (0.007 sec)
    */

    -- 配列で変えさせる
    select
      json_arrayagg(item)
    from
      json_sample
    ;

    /*
    +---------------------------------------------------------------------------------------------------------------------------+
    | json_arrayagg(item)                                                                                                       |
    +---------------------------------------------------------------------------------------------------------------------------+
    | [{"fruit" : ["apple", "peach"]},{"drink" : ["tea", "samovar", "Matcha"]},{"num": "[1,2,3,4]", "double_num": "[2,4,6,8]"}] |
    +---------------------------------------------------------------------------------------------------------------------------+
    1 row in set (0.001 sec)
    */
    ```

    ### 特定のキーに対応する値を抽出

    ```sql
    -- json_extract(column_name, "$.key_name")
    select
      json_extract(item, "$.fruit") from json_sample;

    /*
    +-------------------------------+
    | json_extract(item, "$.fruit") |
    +-------------------------------+
    | ["apple", "peach"]            |
    | NULL                          |
    | NULL                          |
    +-------------------------------+
    3 rows in set (0.001 sec)
    */

    select 
      json_extract(item, "$.fruit", "$.drink", "$.num")
      as list
    from 
      json_sample
    ;

    /*
    +--------------------------------+
    | list                           |
    +--------------------------------+
    | [["apple", "peach"]]           |
    | [["tea", "samovar", "Matcha"]] |
    | ["[1,2,3,4]"]                  |
    +--------------------------------+
    3 rows in set (0.001 sec)
    */
    ```

    ### where句に$.key_nameを使う

    ```sql
    -- 配列の場合、$[0]などでindexを指定できる
    select
      json_extract(json_extract(item, "$.fruit"), "$[0]")
      as first_fruit
    from 
      json_sample
    ;

    /*
    +-------------+
    | first_fruit |
    +-------------+
    | "apple"     |
    | NULL        |
    | NULL        |
    +-------------+
    3 rows in set (0.001 sec)
    */

    -- 上記をwhereに利用
    select
      *
    from
      json_sample
    where
      json_extract(item, "$.fruit[0]") = "apple"
    ;

    /*
    +------+--------------------------------+
    | id   | item                           |
    +------+--------------------------------+
    |    1 | {"fruit" : ["apple", "peach"]} |
    +------+--------------------------------+
    1 row in set (0.001 sec)
    */
    ```

    !!! info

        ### mysqlならもっと簡単にできる

        ```sql
        /*
         ->をJSON_EXTRACT
         ->>をJSON_UNQUOTE(JSON_EXTRACT())として使える
        */
        select *
        from json_sample
        where 
          item->"$.fruit[0]" = "apple"
        ;
        ```
    
    ### JSONに値を追加する

    ```sql
    -- 普通に追加する
    update json_sample
    set 
      item = json_insert(item, "$.action", "sleep")
    where
      id = 2
    ;

    -- 配列について
    select JSON_ARRAY("neko", "inu");

    /*
    +---------------------------+
    | JSON_ARRAY("neko", "inu") |
    +---------------------------+
    | ["neko", "inu"]           |
    +---------------------------+
    1 row in set (0.000 sec)
    */

    -- 配列を追加する
    update json_sample
    set 
      item = json_insert(
              item, 
              "$.animal",
              json_array("neko", "inu"))
    where
      id = 1
    ;

    select * from json_sample;

    /*
    +------+------------------------------------------------------------+
    | id   | item                                                       |
    +------+------------------------------------------------------------+
    |    1 | {"fruit": ["apple", "peach"], "animal": ["neko", "inu"]}   |
    |    2 | {"drink": ["tea", "samovar", "Matcha"], "action": "sleep"} |
    |    3 | {"num": "[1,2,3,4]", "double_num": "[2,4,6,8]"}            |
    +------+------------------------------------------------------------+
    3 rows in set (0.000 sec)
    */
    ```

    ### JSONからkey-valueを削除する

    ```sql
    select
      json_remove('{"A" : 1, "B": [2,3]}', "$.B")

    /*
    +---------------------------------------------+
    | json_remove('{"A" : 1, "B": [2,3]}', "$.B") |
    +---------------------------------------------+
    | {"A": 1}                                    |
    +---------------------------------------------+
    1 row in set (0.001 sec)
    */

    update json_sample
    set
      item = json_remove(item, "$.double_num")
    where
      id = 3
    ;
    -- Query OK, 1 row affected (0.003 sec)
    -- Rows matched: 1  Changed: 1  Warnings: 0

    select * from json_sample;
    /*
    +------+------------------------------------------------------------+
    | id   | item                                                       |
    +------+------------------------------------------------------------+
    |    1 | {"fruit": ["apple", "peach"], "animal": ["neko", "inu"]}   |
    |    2 | {"drink": ["tea", "samovar", "Matcha"], "action": "sleep"} |
    |    3 | {"num": "[1,2,3,4]"}                                       |
    +------+------------------------------------------------------------+
    3 rows in set (0.001 sec)
    */
    ```

    ### 置換する

    ```sql
    /*
    json_insertは挿入のみ
    json_replaceは更新のみ
    json_setならどっちもOKという特徴がある
    */
    update json_sample
    set
      item = json_replace(item, "$.action", "walk")
    where
      id = 2
    ;

    select * from json_sample;
    /*
    +------+-----------------------------------------------------------+
    | id   | item                                                      |
    +------+-----------------------------------------------------------+
    |    1 | {"fruit": ["apple", "peach"], "animal": ["neko", "inu"]}  |
    |    2 | {"drink": ["tea", "samovar", "Matcha"], "action": "walk"} |
    |    3 | {"num": "[1,2,3,4]"}                                      |
    +------+-----------------------------------------------------------+
    3 rows in set (0.000 sec)
    */
    ```

    ### 配列にデータ追加

    ```sql
    update json_sample
      set item = 
        json_array_append(item, "$.drink", "water")
    where
      id = 2
    ;
    /*
    +------+--------------------------------------------------------------------+
    | id   | item                                                               |
    +------+--------------------------------------------------------------------+
    |    1 | {"fruit": ["apple", "peach"], "animal": ["neko", "inu"]}           |
    |    2 | {"drink": ["tea", "samovar", "Matcha", "water"], "action": "walk"} |
    |    3 | {"num": "[1,2,3,4]"}                                               |
    +------+--------------------------------------------------------------------+
    3 rows in set (0.000 sec)
    */
    ```

    ### 検索と配列からのデータ削除

    ```sql
    --　検索
    select json_search(item, "one", "apple") 
    from json_sample;

    /*
    +-----------------------------------+
    | json_search(item, "one", "apple") |
    +-----------------------------------+
    | "$.fruit[0]"                      |
    | NULL                              |
    | NULL                              |
    +-----------------------------------+
    3 rows in set (0.000 sec)
    */

    -- json_unquote -> quoteを取り除く
    -- whereを指定しないと他の行がnullになるので注意

    set @value = "apple";

    update json_sample
    set item = 
      json_remove(
        item,
        json_unquote(json_search(item, "one", @value))
      )
    where id = 1
    ;

    select * from json_sample;
    /*
    +------+--------------------------------------------------------------------+
    | id   | item                                                               |
    +------+--------------------------------------------------------------------+
    |    1 | {"fruit": ["peach"], "animal": ["neko", "inu"]}                    |
    |    2 | {"drink": ["tea", "samovar", "Matcha", "water"], "action": "walk"} |
    |    3 | {"num": "[1,2,3,4]"}                                               |
    +------+--------------------------------------------------------------------+
    3 rows in set (0.001 sec)
    */
    ```


## Q164 JSONをmessagePackに変換する方法について知っていますか?

??? success
    ### JSONからMessagePackへの変換

    ```json
    {
      "a": null,
      "b": 10,
      "c" : [20],
      "d" : "30"
    }
    ```

    ```py
    # pip install msgpack-python
    import json
    import msgpack

    json_file_path = "./json_file.json"
    msg_file_path = "./output.msg"

    #----書き込み処理-------
    with open(json_file_path, "r") as json_file:
      data = json.load(json_file)
    
    packed_data = msgpack.packb(data)

    with open(msg_file_path, "wb") as msg_file:
      msg_file.write(packed_data)
    
    #----読み込んで使う-----
    for msg in msgpack.Unpacker(open(msg_file_path, "rb")):
      print(msg)
    
    # {b'a': None, b'b': 10, b'c': [20], b'd': b'30'}
    ```

    ```bash
    # 変換後の中身を見てみる
    # od -hやoptionなしのhexdumpだと、2byteずつ読み込み
    # 順序を逆転させるため、1byteずつ読み込ませる
    # hexdump -C等でももちろんいい

    cat output.msg | xxd
    # 00000000: 84a1 61c0 a162 0aa1 6391 14a1 64a2 3330

    #----容量が削減できていることも確認可能------

    ls -l | \
    awk '{print $5, "byte : ", $9 }'

    # 66 byte :  json_file.json
    # 16 byte :  output.msg
    ```

## Q165 デバイス、受信サーバ間の通信方式について知っていますか?

??? success
    ### 通常のデータの場合

    ```text
    ・社内DBにあるデータや、パブリックデータ
      ⇒既に集まっている

    ・WebサイトログやSNSデータ
      ⇒Webサイトログなら、ログを
        SNSデータなら、提供しているAPIを
        許可されているなら、スクレイピングを使う
    ```

    ### IoT受信サーバでデバイスと通信する場合

    ```text
    ・通信の方法は以下の様になる
    
    HTTPプロトコル
      ・通常のWebシステムと同様に、HTTPプロトコルを利用した
      　Web APIを用いてデバイスからアクセス
      ・たとえば、GET/POST
    
    WebSocket
      ・音声や動画のリアルタイム通信を行う
      ・HTTPプロトコルと比べ、接続を継続したままに
        しておけるという利点がある
    
    MQTT
      ・第三の媒体を介して、キュー方式を利用する
      ・1つのパブリッシャーから多数のサブスクライバーへの
        通信が可能
      ・サービス品質を選ぶことができる
      ・受信サーバ側がダウンしていても、
        情報はキューに残り続けるので、データの損失が防げる
    ```

## Q166 処理サーバでのデータ処理方法と、クレンジングについて知っていますか?

??? success
    ### 前提

    ```text
    ・蓄積サーバにデータを格納する前に、
      処理サーバでデータの加工が行われる
    ```

    ### 2つの処理方法

    ```text
    ストリーム処理
      ・処理サーバに到着したデータを逐次処理する方法
      ・リアルタイムで処理すべきデータに使う
    
    Spark Stremaing
      
    ```
