# クイズ(17:BigData)

## Q151 ビッグデータという概念について知っていますか?

??? success
    ### ビッグデータ概要(3つのV)

    ```text
    Volume(データの量)
      ・数十億や数兆単位のデータがあるか
    
    Velocity(データの速度)
      ・データがどのくらい素早く届くか
      ・速いというのは、データが更新される頻度
      （常に変化し続けるデータで、現在に即した結果を導く）
    
    Variety(データの多様性)
      ・データが常に構造化されているとは限らず、
        非構造化データとして提供される
    
    これを踏まえてビッグデータとは
      ・従来のシステムでは処理できないほど巨大なデータ
      ・従来の構造に収まらない複雑なデータ
      ・常に変化し続けるデータ


    最近はあと2つのVもある
      Veracity(正確性)
      Value（価値）⇒データを組み合わせて新たな価値を生み出す
    
    ```

    ###　ビッグデータの種類

    ```text
    構造化データ
      ・従来のRDBに格納できるデータ
    
    準構造化データ
      ・ログやJSONなどの完全な構造定義を持たないデータ
    
    非構造化データ
      ・文書、音声、動画、画像など、データ部に構造定義を
        まったくもたないデータ
    ```

    ### なぜビッグデータが登場してきたのか

    ```text
    [技術の発展]
      ・分散処理技術(Hadoop)やクラウドサービスの充実
      ・NoSQL-DB, 機械学習などの発展
    
    [データの増加]
      ・IoT家電の登場や、SNS、Webシステムなどにより
        生成されるデータが以前より増えた
    
    [低価格化]
      ・データを取得し、送信するセンサーなどの低価格化
      ・クラウドなどもここに含まれる
    ```

    ### ビッグデータの格納場所

    ```text
    [自社]
      ・自社の基幹システム
      ・Web,SNSサービスなどの操作ログ等
    
    [社外]
      ・スマホ/家電の使用量などのデータ
      ・企業間で共有されるデータ
    
    [一般]
      ・政府や自治体などで公開されたデータ
      ・データ提供事業者から入手したデータ
    ```

    ### オープンデータ

    ```text
    [特徴]
    ・誰でも入手可能で、利用/再配布も可能なデータ
    ・コンピュータから利用可能
    ・特許/著作権周りの問題もない

    ⇒上記の条件に当てはまるならオープンデータ

    [公開している機関]
    ・政府、地方自治体、研究機関、大学、民間企業

    [オープンデータの例]
    ・国勢調査データ
    ・気象データ

    [使用例]
    ・気象データと、自社の販売データの相関関係を考える
    ・多変量解析周りの、統計的な知識が必要。
    ・因子分析等の知識がないと結果を違える

    [結果を違える例]
    ・1950年頃の過去データを用い、採用活動に用いる
    ⇒人種間や性別間の偏見が残ってしまう

    ```

    ### 各分野におけるビッグデータ

    ```text
    マーケティング
      ・Webのログ(準構造データ)やカメラ(非構造データ)
        から顧客の行動を分析
    
    製品開発
      ・センサ（準構造データ）や顧客の声（非構造データ）
        をもとに、開発の方針を決定
    
    コンプライアンス
      ・正しい文章と、誤った文章(非構造データ)を元に
        学習させ、コサイン類似度などで、不適切な文言をはじく

    セキュリティ
      ・サイバー攻撃のパターンをログ(準構造データ)
      　などから検知して、予防する

    メンテナンス
      ・センサデータの故障予測を行う
    
    社会インフラ
      ・過去のデータをもとに、災害の予測を立てる
    ```

    ### BigDataを使ったシステムについて具体案を出してみる

    ??? info

        ```text

        ・実際に病院で診断を下された患者(考えすぎも含む)が、
          Web上でどのような検索ワードを用いて
          自身の病状を検索していたのか
          (もちろん、個人を識別できるような情報は削除する)

        -----------------------

        ・現在のWebにおいて、医療関係の情報は
          人を不安にさせるものが多い
        ・たとえば、鼻水が出ると調べれば、
          脳みそがジュースになっているという検索結果が出るし、
          元気がないと調べれば、筋ジスだと返ってくる

        (ちなみに私は、スギ花粉による花粉症に
          毎年春先苦しめられているが、
          以前「AI診断」と検索して出てきたサービスに
          自身の症状を入力したところ、
          「今すぐ病院に行くべきで、難病にかかっている
          可能性がある」と言われた。控え目に言って糞だ)
        
        ・この結果、患者と医師はどうなるか

        ・患者の場合、不安に駆られる。
          年がら年中身体のことを考えることで、
          病院に行く回数は増えるだろうが、
          これは健康とは言えないだろう。精神的に病んでいる

        ・また、現場の医師は明らかに不利益を被る
        　ネットで調べ、不安になった人間が病院を訪れ
          自身は重い病気なんだと主張するからだ
          
        ・そして、この手の患者は
          医師に大丈夫ですと言われても安心しない。
          仮に一時心の平穏を取り戻せたとしても、
          数日後には別の箇所が心配になり、また調べ、
          そして不安になり、医師に相談することになる。
          この繰り返しだ。これが死ぬまで続く。
        
        ・つまるところ、ネットにあふれる健康情報は
        　百害あって一利ない。

        ・そして、一利ないということを患者に分からせるためには
          実際のデータを持ってくるのが一番である

        ・「鼻水　水っぽい　一日中」と調べた患者のうち、
          何%が、脳みそがジュースになっている病気だったのか。
          そして、何%が、花粉症だったのか。
        
        ・小さなしこりがあると調べた患者のうち、
          何％が悪性腫瘍で、何%が脂肪種だったのか。
        
        ・是非、問診表の中で、
          ここに来るまでにネットで病気に関するワードを
          打ち込んだか記載させ、
          実際の病状と照らし合わせてほしい。
          患者の心の平穏にも、現場の医師のためにもなるはずだ。

        ※これは出所のはっきりしている情報
        （たとえば医師が書いているサイト）にも言える話である

          大体、責任問題になるのだから、医師が絶対に大丈夫
          というわけがない。「一度診断を受けるべき」
          気軽な気持ちで、こう締めくくるわけだ。
          で、心気症に近い患者は、違う部位が気になるたびに
          「一度診断を受けるべき」と思い、病院を訪れ
          病院はパンクする。私はこの悪循環をなくしたい

        ```
    
## Q152 個人情報とビッグデータの関係性について知っていますか?

??? success
    ### データ収集と個人情報

    ```text
    ・ビッグデータを用いたデータ分析が行いたいなら
      まずはデータが必要

    ・データを収集するとして、
      個人情報保護法等を守る必要がある

    ・また特定の国に限らず、情報収集をする場合は、
      該当国・地域の法律も順守する(GDPRとかね)
    ```

    ### 個人情報関連

    ```text
    個人情報
      ・個人(生存者のみ)の氏名、生年月日、その他の記述により
        特定の個人を識別できるような情報
    
    個人情報取扱事業者
      ・個人情報をDB等で所持し、事業に用いている事業者
      ・取扱件数にかかわらない(2015年から)
      ・流失の際、報告や改善措置を怠ると刑事罰の対象になる
    
    個人情報取得の際には
      ・オプトイン
      　⇒（利用規約などで、予め本人に利用目的を明示して
            同意のチェックを付けさせる）

      ・オプトアウト
        ⇒（拒否しない限り、同意とみなす
            Webサイトのクッキーなどに多い）

      ・情報を第三者に提供する場合については、
      　通常予め同意を取っておく必要がある(オプトイン)
      
      ・オプトアウト方式で第三者に情報提供する場合、
        プライバシーポリシー等に必要事項を記載し、
        個人情報保護委員会にその旨を申し立てる必要がある

      ・2022年からは開示請求で、第三者にどのように提供された
      　のか確かめられるようになった
      
    
    要配慮情報について
      ・人種、信条、病歴などの要配慮情報は
        オプトアウト方式では第三者に提供できない
      
    ```

## Q153 匿名加工処理について知っていますか?

??? success
    ### 摂動法とは

    ```text
    ・データの加工や変換において、
      元のデータを特定の方法で変更し、
      元に戻す事が出来ないようにする技術
    
    ・プライバシー保護やデータの安全性の観点から行われる

    ・暗号法と異なり、解読することができないので
      正しく処理すれば復元することは困難になる

    ※そもそも摂動とは
      ・力学の言葉で、乱すこと
      ・セキュリティでいうと、ランダムノイズ等を書けて
        元のデータを分からなくするように乱している
    ```

    ### 前提（識別子/準識別子）

    ```text
    識別子
      ・個人をはっきり指し示す属性（マイナンバーとか）
    
    準識別子
      ・年齢、性別、居住地など組み合わせることで
        個人の特定が可能になる属性
      
    要配慮情報
      ・センシティブ属性とも呼ぶ
      ・人に知られたくないことは全部そう
      ・年収とかも
    
    ⇒要配慮情報と個人を結び付けられないよう匿名化を行う
    ```

## Q154 K-匿名化について知っていますか?

??? success
    ### K-匿名化(摂動法)

    ```text
    ・同じような人が、必ずK人以上いる状態を作ること
      （準識別子が全く同一の個人が少なくともK人存在）

    [抑制]
      ・特定の属性の値を*で変換
      ・識別子は必ず変換
      ・必要であれば準識別子も
    
    [一般化]
      ・97歳を90代などに変換
      ・個々の属性値を広い範囲に置換する
    
    1 抑制、一般化を行ったうえで、準識別子の組み合わせを
      調べる
    
    2 どの組み合わせを用いても、K個以上のレコードが
      残る場合、K-匿名性を満たすという
    
    [弱点：一般的に]
      ・ランダム性を含まないため、推測が可能

    [弱点:背景知識攻撃]
      ・攻撃者が何らかの背景知識を持っている場合、
      （たとえば要配慮情報の中身を大まかに知っている場合）
      　明らかに当てはまらないものを除くことで、
      　個人が特定される可能性がある
    
    [弱点：同種攻撃]
      ・要配慮情報を含めたすべての機密属性が同じ場合、
        レコードを特定できなくとも、機密情報がばれる
    ```

    ```sql
    -- 前処理前
    -- 宗教を要配慮情報とした場合

    select 
      "Nekotaro" name,
      38 age,
      1 sex,
      "Tokyo" region,
      "Nekozyara-Kyo" belief
    union all
    select 
      "Torakiti" name,
      32 age,
      1 sex,
      "Tokyo" region,
      "Nekozyara-kyo" belief
    union all
    select 
      "LionMaru" name,
      59 age,
      1 sex,
      "Osaka" region,
      "Tategami-Kyo" belief
    union all
    select 
      "Kabanosuke" name,
      52 age,
      1 sex,
      "Osaka" region,
      "Mizuabi-Club" belief
    ;

    /*
    +------------+-----+-----+--------+---------------+
    | name       | age | sex | region | belief        |
    +------------+-----+-----+--------+---------------+
    | Nekotaro   |  38 |   1 | Tokyo  | Nekozyara-Kyo |
    | Torakiti   |  32 |   1 | Tokyo  | Nekozyara-kyo |
    | LionMaru   |  59 |   1 | Osaka  | Tategami-Kyo  |
    | Kabanosuke |  52 |   1 | Osaka  | Mizuabi-Club  |
    +------------+-----+-----+--------+---------------+
    4 rows in set (0.000 sec)
    */

    -- K-多様性適用後
    select 
      "*" name,
      "30s-50s" age,
      1 sex,
      "Japan" region,
      "Nekozyara-Kyo" belief
    union all
    select 
      "*" name,
      "30s-50s" age,
      1 sex,
      "Japan" region,
      "Nekozyara-kyo" belief
    union all
    select 
      "*" name,
      "30s-50s" age,
      1 sex,
      "Japan" region,
      "Tategami-Kyo" belief
    union all
    select 
      "*" name,
      "30s-50s" age,
      1 sex,
      "Japan" region,
      "Mizuabi-Club" belief
    ;

    /*
    準識別子をage,sex,regionとすると、
    2-匿名性が担保されているが......

    30代と判明した場合、同種攻撃で突破されることが分かる
    */

    /*
    +------+-----+-----+--------+---------------+
    | name | age | sex | region | belief        |
    +------+-----+-----+--------+---------------+
    | *    | 30s |   1 | Tokyo  | Nekozyara-Kyo |
    | *    | 30s |   1 | Tokyo  | Nekozyara-kyo |
    | *    | 50s |   1 | Osaka  | Tategami-Kyo  |
    | *    | 50s |   1 | Osaka  | Mizuabi-Club  |
    +------+-----+-----+--------+---------------+
    4 rows in set (0.001 sec)
    */
    ```

## Q155 L-多様性について知っていますか?

??? success

    ### L-多様性(摂動法)

    ```text
    ・漏えいさせたくない属性が、
      同一グループ内で、
      少なくとも「L種類以上ある」状態を作ること
    
    ※同一グループ：準識別子が全て同じ集まり

    ⇒L=2以上の場合、同値攻撃も不可
    ⇒準識別子を照らし合わせても、個人の情報を特定できなくなる
    
    [Lを増やすには]
      ・一般化の手法を見直して、少数のグループをなくす

      ・たとえば以下の例の場合、ageとregionの幅を
        大きくすることで、Lを増加させている
      
      ・どうしてもLが広げられない場合は、
        そこだけ*で匿名化する方法もある
    
    ・以下の場合、L = 1である
      +------+-----+-----+--------+---------------+
      | name | age | sex | region | belief        |
      +------+-----+-----+--------+---------------+
      | *    | 30s |   1 | Tokyo  | Nekozyara-Kyo |
      | *    | 30s |   1 | Tokyo  | Nekozyara-kyo |
      | *    | 50s |   1 | Osaka  | Tategami-Kyo  |
      | *    | 50s |   1 | Osaka  | Mizuabi-Club  |
      +------+-----+-----+--------+---------------+
      4 rows in set (0.001 sec)
    
    ・これはL = 3
    +------+---------+-----+--------+---------------+
    | name | age     | sex | region | belief        |
    +------+---------+-----+--------+---------------+
    | *    | 30s-50s |   1 | Japan  | Nekozyara-Kyo |
    | *    | 30s-50s |   1 | Japan  | Nekozyara-kyo |
    | *    | 30s-50s |   1 | Japan  | Tategami-Kyo  |
    | *    | 30s-50s |   1 | Japan  | Mizuabi-Club  |
    +------+---------+-----+--------+---------------+
    4 rows in set (0.001 sec)  
  
    ```

## Q156 T-近接性について知っていますか?

??? success

    ### T-近接性(摂動法)

    ```text
    [概要]
    ・任意の準識別子グループ内の要配慮情報の分布と、
      全体の要配慮情報との分布の差がt以下である
    
    [k-匿名化やL-多様性との違い]
      ・これら二つは個数の下限を考えていた
      ・t-近接性は、差の上限を保証する
    
    [t-近接性を満たさない例]
      ・データセット全体の場合、収入の分布は散らばっている
      ・特定のグループには、特定範囲の収入情報しか入っていない
    ⇒攻撃者は特定の準識別子を持った人の傾向を
      知ることができてしまう

    [t-近接性を満たす例]
      ・データセット全体の場合、収入の分布は散らばっている
      ・特定のグループを見ても、収入の分布は変わらない
      ⇒攻撃者は準識別子を頼りに探しても、
        全体の分布以上の特徴を知ることができない
    
    [t-近接性のデメリット]
      ・データ利用者の得られる情報も制限してしまうので
        データを価値あるものとして扱うことが難しくなる
    ```

## Q157 差分プライバシについて知っていますか?

??? success

    ### 差分プライバシ(摂動法)

    ```text
    [概要]
      ・データ全体ではなく、クエリに対して加工を行う

      ・どれだけ他人と見分けがつかないかを、ε(プライバシー強度)
        で表現する
    
    [モザイク効果/再構築攻撃への対策]
        ・モザイク効果は、1つでは安全そうに思えたデータセットが
          データセットを複数重ね合わせたとき、
          プライバシーの暴露を引き起こす事象を指す
        
        ・また、複数のデータを重ね合わせて、
          個人の情報を特定する攻撃を、再構築攻撃という
        
        ・ここでk-匿名化などの手法が、単一のデータセットに
          終始していたことを思い出す。
        
        ・つまり、k-匿名化などの手法は、一見安全そうに見えても
          再構築攻撃による潜在的な危険に対応できない
        
        ・差分プライバシは、統計的な尺度を導入することで、
          これらの問題点や潜在的な危険に対応する
      
    [差分プライバシの考え方]
      ・プライバシー保護を施した特定のqueryに対し、
        どれくらいプライバシーが損なわれるかをε(>=0)で表現する
      
      ・値が大きいほど、プライバシーが漏洩する

      ・特定のqueryのプライバシー損失がε以下の場合、
        queryは、ε-差分プライバシー(ε-DP)を満たすという
      
      ・具体的には、以下2つのデータベースについて考える
      　D1：Aさんのデータが含まれるデータベース
        D2: Aさんのデータを他の誰かの物に置き換えたデータベース
        (一行だけ異なる2つのDBを隣接データベースという)
        (※なお、置き換えるのではなく、除くこともある)
      
      ・単純にクエリを適用すると、その差分から
        Aさんの個人情報が導き出されてしまうため
      　何らかの確率分布をもとにランダム化関数を適用し
      　結果クエリを算出する
      

      ・その結果、見分けがつかなければ、D1とD2の結果クエリから
      　Aさんの情報を入手することができないといえ、
        ε-DPは小さくなる。逆もまた然り
      
    [何らかの確率分布?]
      ・Laplace分布が有名
      ・分散が大きくなれば、それだけ特定しにくくなるため
        εは小さくなる
    
    [デメリット]
      ・プライバシーの消費は累積し、続ければ効果がなくなる
      ・分散が大きくなるということは、精度が下がる事を意味する
    
    [プライバシ以外のメリット]
      ・データそのものを抑制したり、一般化を行うk-匿名化と比べ
      　差分プライバシは個々のクエリに誤差を加えただけなので、
        統計的な傾向は守られる

        ⇒統計的分析に使用できる
      
      
    ```

## Q158 準同型暗号方式について知っていますか?

??? success

    ### 準同型暗号方式(暗号法)

    ```text
    [秘密計算]
      ・データを暗号化したまま計算できる技術を指す
      ・機密データを送受信する過程で、
        復号、暗号化を繰り返す必要がなくなる
    
    [準同型暗号方式]
      1 公開鍵を利用し、複雑な暗号関数を作る
      2 暗号化した状態でデータの計算を委託(計算鍵も同封)
      3 委託先の企業は暗号化した状態で計算(計算鍵を利用)
      4 暗号化した結果を委託元に戻す
      5 委託元の企業は、復号用の秘密鍵を持っているため、
        それを利用してデータの内容が判定できる
    
    [準同型暗号方式のデメリット]
      ・暗号化したデータが巨大になり、コストがかかる
      ・鍵が流出すると終わる
      ・鍵をなくすと、復号できなくなる
    ```

## Q159 秘密分散方式について知っていますか?

??? success

    ### 秘密分散方式(暗号化)

    ```text
    [秘密分散方式]
      秘匿情報を分割して、保管しておくこと
    
    [(k,n)しきい値法 -> 秘密分散方式の一つ]
      1 データをn個の断片（シェア）に分割する
      2 n個のうち、k個以上が集まれば復元可能
      2 シェアは、複数のサーバに1つずつ分散させる
      3 委託先は、シャアのうちk個未満の情報を集め
        処理を行い、結果を出力
      4 委託元は、分散した結果をすべて集め、k個以上
        集めることで復元する
    
    [メリット]
      ・k-1個のサーバまでなら、侵入されても
      　何の情報も得られないため機密性において軍配があがる

      ・n-k個までなら故障しても、残りのコンピュータから
        データを復元可能

    [分割手法について(一例)]
      ・k-1次多項式の曲線を特定するには、
      　k個のデータ点があればいい。

      ・これを利用して、k個のシェアから、k-1次多項式を解き、
        秘密情報である切片を入手する
      ⇒シャミアの秘密分散法
    ```

## Q160 ビッグデータとクラウドの関係性について知っていますか?

??? success
    ### クラウドサービスの種類

    ```text
    SaaS(Software as a Service)
      ・インターネットを経由して、ソフトウェアパッケージを提供
        するサービス
    
    PaaS(Platform as a Service)
      ・インターネットを経由して、アプリの開発・運用環境全体
        を提供するサービス
    
    IaaS(Infrastructure as a Service)
      ・昔はHaaS(Hardware as a Service)と呼ばれていたことも
      ・インターネントを経由して、ハードウェアや回線などの
        インフラを提供するサービス
      ・ユーザはハードウェアを物理的に所有せずに済む
    ```

    ### クラウドサービスで大規模データを扱う利点

    ```text
    コスト面
      ・先行投資（ハードウェアの準備等）が不要

      ・自動でスケーリングを行い、繁忙期や時間帯に合わせて、
        処理能力を変更でき、コスト削減できる

    -------------


    安全性
      ・複数のAZやリージョンに、データを複製しているため
        耐障害性という面で優れている
    
    -------------


    運用面
      ・ビッグデータは基本的にデータが日増しになっていく
        クラウドなら、容量拡大に伴う作業が不要
    
    -------------

    人的資源
      ・インフラの構築に割いていた時間を、
        本当に必要な業務に回すことができる

    -------------

    独自のカスタマイズができない
      ・企業独自のカスタマイズをさせられずに済むのは
        転職が多い業界にとってはむしろメリットだろう

      ・下手に必要以上のカスタマイズができてしまうと、
        却って本来の解決策が見つけにくくなるかもしれない
    ```

    ### 欠点

    ```text
    世界規模の障害が起こった場合
      ・クラウドサービスで障害が起こると、
        業務に影響が出る場合がある
    ```

## Q161 ビッグデータとIoTの関係性について知っていますか?

??? success

    ### 基本

    ```text
    「モノ」から取得したデータをビッグデータの一部として処理し、
    統計的分析や、機械学習で用いる
    ```

    ### IoT関連ワード概要

    ```text
    Iot(Internet of Things)
      ・モノがインターネットに接続できるようになった状態

      ・20世紀まではインターネットとは関係のなかったもの
        たとえば家電や、時計、眼鏡、自動車などが
        ネットワークにつながり、相互に情報交換が
        可能になった状態を意味する
      
      ・言葉ができた90年代当初は
        単にRFID(radio frequency identification)のことを
        指していたが、今は上の定義が正しい

    -------------

    ユビキタス社会という考え方
      ・そもそもubiquitousがいたるところにという意味
      ・つまりいつでもどこでもinternetを利用できる社会を指す
      ・IoTとの違いは人視点であること
    
    --------------

    M2Mという言葉について
      ・Machine to Machine
      ・機械同士が、人を介在しないで情報をやり取りするシステム
      ・必ずしもインターネントを介す必要はない
    
    ⇒つまり、IoTはユビキタス社会とM2Mを包括する
    ```

    ### 取得したデータを用いてデータ分析するまでの流れ

    ```text
    ※用語については下で補完する

    1 デバイスに埋め込んだセンサを用いて、データを取得する

    2 デバイスから受信サーバへとデータ送信
    
    3 処理サーバがストリーム方式やバッチ処理で
      データを加工する。(ETLの場合)
    ⇒リアルタイムで情報が必要なら、デバイスに送り返す
    
    4 (加工済みの)データを、蓄積サーバで保管

    6 必要に応じ、統計的手法や、機械学習を用いて分析を行う
      ⇒ELTなら前処理として加工を行う
    ```

    ### 上記の用語補足

    ```text
    センサ
      ・物理的な現象を検知して、電気信号として出力する装置
    
    デバイス
      ・センサを介して、ネットワークに接続された装置、モノ

    受信サーバ
      ・HTTP/MQTT/WebSocket等の通信方法を用い、
      　デバイスとのデータの送受信を行う

    処理サーバ
      ・ETLツールなどを用い、クレンジングを行う
    
    クレンジング
      ・データの整理や加工を行うこと
    
    蓄積サーバ
      ・ビッグデータを保管するサーバ
    ```

    ### なぜ受信/処理/蓄積サーバを1つにまとめないのか

    ```text
    ・各サーバが異なる役割を持つから

    受信サーバ
      ・データ転送速度等を最適化
    
    処理サーバ
      ・計算負荷に対応するために強力なCPUが必要
      ・大量のメモリもいる
    
    蓄積サーバ
      ・大量のデータを保管するために、大容量ストレージが必要
    
    各サーバを分けることで
      ・それぞれをスケールアップ/スケールアウト可能
      ・システムを柔軟に組むことができる
    ```

    ### センサ補足

    ```text
    概要
      ・一つのデバイスに対して、
        複数のセンサが埋め込まれる事が多い

    
    画像センサ
      ・画像や動画を作成
      ・赤外線を検知して、画像処理するものもある
      ・工場の欠けている部品をチェックしたり

    光センサ
      ・光の強度を測定する
      ・スマホで言うと、画面の明るさの自動調整機能
    
    温度/湿度センサ
      ・温度, 湿度を測定する
      ・エアコンの自動調整等に使う
    
    振動/速度/加速度センサ
      ・機器の振動, 速度, 加速度の測定
      ・自動車などには不可欠だろう
    
    地磁気センサ
      ・地磁気を検出することで方角を計測
      ・カーナビやGPS端末の電子コンパスとしても使用される
    
    ジャイロセンサ
      ・デバイスの傾きを検知する
      ・昔のカービィのゲームであったな
    
    音声マイク
      ・機器が発する音や、人の声などの音声を収集する
      ・カメラの録音機能や、工場の異音検知等はこれ
    
    ```

    ### デバイス補足

    ```text
    デバイスの機能
      ・センシング
      ・フィードバック
    
    センシング
      ・センサーを利用して、デバイス本体や周囲環境を収集し、
        IoTシステムに通知すること
      
    フィードバック
      ・システムからの通知を受け、指示や動作を
        機器に返すこと
    ```

    ### センサが取得するデータのフォーマット

    ```text
    XML
      ・人が呼んで分かりやすい形式
      ・またはデータ量が多い場合に使用
      ⇒SQLでの使用法については次の節で行う
    
    JSON
      ・データ量が少ない場合に使用
      ⇒SQLでの使用法については次の節で扱う
    
    MessagePack
      ・バイナリデータをそのまま扱いたい場合に使用
      ・パースする必要がない
      ・形式自体はJSONに似ている
      ・シリアライズ、デシリアライズともに非常に高速
      ・データサイズは小さく、ストリーム処理が可能な特徴を持つ
    ```
## Q162 XMLをSQLで扱う方法を知っていますか?

??? success

    ### XMLファイルをSQLのテーブルとして扱う

    ```sql
    use sakila;

    create table person (
      person_id int not null primary key,
      fname varchar(40) null,
      lname varchar(40) null,
      created timestamp
    );
    ```

    ```xml
    <!-- test.xml -->
    <!-- 仮にtableに対応するカラムがない場合は、skipされる-->
    <!-- fieldの指定順序は関係ない -->
    <list>
      <person person_id="1" fname="Neko" lname="Taro" />
      <person person_id="2" fname="Inu" lname="Ziro" />
      <person person_id="3">
        <fname>Tora</fname>
        <lname>Kuroo</lname>
      </person>
      <person>
        <field name="person_id">4</field>
        <field name="fname">Onigawara</field>
        <field name="lname">Momizi</field>
      </person>
    </list>
    ```

    ```sql
    -- resultsetやrow等の目印がないため
    -- どこからがrowなのかを示すため3行目が必要
    
    /*
    local
      ・MySQLサーバは一時ファイルを格納するディレクトリに
        ファイルのコピーを作成する
      ・これがない場合、ファイルはサーバホストからしか読めない
    */
    load xml local infile "test.xml"
      into table person
      rows identified by "<person>"
    ;

    select * from person;

    /*
    +-----------+-----------+--------+---------------------+
    | person_id | fname     | lname  | created             |
    +-----------+-----------+--------+---------------------+
    |         1 | Neko      | Taro   | 2024-04-13 16:43:31 |
    |         2 | Inu       | Ziro   | 2024-04-13 16:43:31 |
    |         3 | Tora      | Kuroo  | 2024-04-13 16:43:31 |
    |         4 | Onigawara | Momizi | 2024-04-13 16:43:31 |
    +-----------+-----------+--------+---------------------+
    4 rows in set (0.001 sec)
    */
    ```

    ### SQLのテーブルをXMLに変換する

    ```bash
    mysql -unekoinu -pnyanko  \
    --xml -e "select * from sakila.person" \
    > person-dump.xml

    cat person-dump.xml
    ```

    ```xml
    <!-- person-dump.xml -->

    <?xml version="1.0"?>

    <resultset statement="select * from sakila.person
    " xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
      <row>
            <field name="person_id">1</field>
            <field name="fname">Neko</field>
            <field name="lname">Taro</field>
            <field name="created">2024-04-13 16:43:31</field>        
      </row>

      <row>
            <field name="person_id">2</field>
            <field name="fname">Inu</field>
            <field name="lname">Ziro</field>
            <field name="created">2024-04-13 16:43:31</field>        
      </row>

      <row>
            <field name="person_id">3</field>
            <field name="fname">Tora</field>
            <field name="lname">Kuroo</field>
            <field name="created">2024-04-13 16:43:31</field>        
      </row>

      <row>
            <field name="person_id">4</field>
            <field name="fname">Onigawara</field>
            <field name="lname">Momizi</field>
            <field name="created">2024-04-13 16:43:31</field>        
      </row>
    </resultset>
    ```

    ```sql
    --　出力結果をさらにtableに
    create table person2 like person;

    --resultsetやrowがあるので、row句は短縮できる
    load xml local infile "person-dump.xml"
    into table person2;

    /*
    +-----------+-----------+--------+---------------------+
    | person_id | fname     | lname  | created             |
    +-----------+-----------+--------+---------------------+
    |         1 | Neko      | Taro   | 2024-04-13 16:43:31 |
    |         2 | Inu       | Ziro   | 2024-04-13 16:43:31 |
    |         3 | Tora      | Kuroo  | 2024-04-13 16:43:31 |
    |         4 | Onigawara | Momizi | 2024-04-13 16:43:31 |
    +-----------+-----------+--------+---------------------+
    4 rows in set (0.001 sec)
    */
    ```

    ### フィールド名を、カラム名と結び付けたい場合

    ```sql
    -- 同じフィールドを使うのは汎用性が低い

    create table individual (
      individual_id int not null primary key,
      name1 varchar(40) null,
      name2 varchar(40) null,
      made timestamp
    );

    -- fieldの値をユーザ変数とし、setを利用して、格納
    load xml local infile "./person-dump.xml"
      into table individual (
        @person_id, @fname, @lname, @created
      )
      set 
        individual_id = @person_id,
        name1 = @fname,
        name2 = @lname,
        made = @created
    ;

    select individual_id, name1, name2  from individual;

    /*
    +---------------+-----------+--------+
    | individual_id | name1     | name2  |
    +---------------+-----------+--------+
    |             1 | Neko      | Taro   |
    |             2 | Inu       | Ziro   |
    |             3 | Tora      | Kuroo  |
    |             4 | Onigawara | Momizi |
    +---------------+-----------+--------+
    4 rows in set (0.001 sec)
    */
    ```

## Q163 JSONをSQLで扱う方法を知っていますか?

??? success
    ### 行挿入

    ```sql
    /*型はJSON*/
    create table json_sample(
      id int, 
      item json
    );

    /*json_objectの形か、下記の様に挿入*/
    insert into json_sample values
    (1, '{"fruit" : ["apple", "peach"]}'),
    (2, '{"drink" : ["tea", "samovar", "Matcha"]}')
    ;

    insert into json_sample values
    (
      3, (
        JSON_OBJECT(
          'num', '[1,2,3,4]', 
          'double_num', '[2,4,6,8]'
        )
      )
    );

    select * from json_sample;
    /*
    +------+-------------------------------------------------+
    | id   | item                                            |
    +------+-------------------------------------------------+   
    |    1 | {"fruit" : ["apple", "peach"]}                  |   
    |    2 | {"drink" : ["tea", "samovar", "Matcha"]}        |   
    |    3 | {"num": "[1,2,3,4]", "double_num": "[2,4,6,8]"} |   
    +------+-------------------------------------------------+   
    3 rows in set (0.001 sec)
    */
    ```

    ### 集約

    ```sql
    select group_concat(item) from json_sample;
    /*
    +-------------------------------------------------------------------------------------------------------------------------+
    | group_concat(item)                                                                                                      |
    +-------------------------------------------------------------------------------------------------------------------------+
    | {"fruit" : ["apple", "peach"]},{"drink" : ["tea", "samovar", "Matcha"]},{"num": "[1,2,3,4]", "double_num": "[2,4,6,8]"} |
    +-------------------------------------------------------------------------------------------------------------------------+
    1 row in set (0.007 sec)
    */

    -- 配列で変えさせる
    select
      json_arrayagg(item)
    from
      json_sample
    ;

    /*
    +---------------------------------------------------------------------------------------------------------------------------+
    | json_arrayagg(item)                                                                                                       |
    +---------------------------------------------------------------------------------------------------------------------------+
    | [{"fruit" : ["apple", "peach"]},{"drink" : ["tea", "samovar", "Matcha"]},{"num": "[1,2,3,4]", "double_num": "[2,4,6,8]"}] |
    +---------------------------------------------------------------------------------------------------------------------------+
    1 row in set (0.001 sec)
    */
    ```

    ### 特定のキーに対応する値を抽出

    ```sql
    -- json_extract(column_name, "$.key_name")
    select
      json_extract(item, "$.fruit") from json_sample;

    /*
    +-------------------------------+
    | json_extract(item, "$.fruit") |
    +-------------------------------+
    | ["apple", "peach"]            |
    | NULL                          |
    | NULL                          |
    +-------------------------------+
    3 rows in set (0.001 sec)
    */

    select 
      json_extract(item, "$.fruit", "$.drink", "$.num")
      as list
    from 
      json_sample
    ;

    /*
    +--------------------------------+
    | list                           |
    +--------------------------------+
    | [["apple", "peach"]]           |
    | [["tea", "samovar", "Matcha"]] |
    | ["[1,2,3,4]"]                  |
    +--------------------------------+
    3 rows in set (0.001 sec)
    */
    ```

    ### where句に$.key_nameを使う

    ```sql
    -- 配列の場合、$[0]などでindexを指定できる
    select
      json_extract(json_extract(item, "$.fruit"), "$[0]")
      as first_fruit
    from 
      json_sample
    ;

    /*
    +-------------+
    | first_fruit |
    +-------------+
    | "apple"     |
    | NULL        |
    | NULL        |
    +-------------+
    3 rows in set (0.001 sec)
    */

    -- 上記をwhereに利用
    select
      *
    from
      json_sample
    where
      json_extract(item, "$.fruit[0]") = "apple"
    ;

    /*
    +------+--------------------------------+
    | id   | item                           |
    +------+--------------------------------+
    |    1 | {"fruit" : ["apple", "peach"]} |
    +------+--------------------------------+
    1 row in set (0.001 sec)
    */
    ```

    !!! info

        ### mysqlならもっと簡単にできる

        ```sql
        /*
         ->をJSON_EXTRACT
         ->>をJSON_UNQUOTE(JSON_EXTRACT())として使える
        */
        select *
        from json_sample
        where 
          item->"$.fruit[0]" = "apple"
        ;
        ```
    
    ### JSONに値を追加する

    ```sql
    -- 普通に追加する
    update json_sample
    set 
      item = json_insert(item, "$.action", "sleep")
    where
      id = 2
    ;

    -- 配列について
    select JSON_ARRAY("neko", "inu");

    /*
    +---------------------------+
    | JSON_ARRAY("neko", "inu") |
    +---------------------------+
    | ["neko", "inu"]           |
    +---------------------------+
    1 row in set (0.000 sec)
    */

    -- 配列を追加する
    update json_sample
    set 
      item = json_insert(
              item, 
              "$.animal",
              json_array("neko", "inu"))
    where
      id = 1
    ;

    select * from json_sample;

    /*
    +------+------------------------------------------------------------+
    | id   | item                                                       |
    +------+------------------------------------------------------------+
    |    1 | {"fruit": ["apple", "peach"], "animal": ["neko", "inu"]}   |
    |    2 | {"drink": ["tea", "samovar", "Matcha"], "action": "sleep"} |
    |    3 | {"num": "[1,2,3,4]", "double_num": "[2,4,6,8]"}            |
    +------+------------------------------------------------------------+
    3 rows in set (0.000 sec)
    */
    ```

    ### JSONからkey-valueを削除する

    ```sql
    select
      json_remove('{"A" : 1, "B": [2,3]}', "$.B")

    /*
    +---------------------------------------------+
    | json_remove('{"A" : 1, "B": [2,3]}', "$.B") |
    +---------------------------------------------+
    | {"A": 1}                                    |
    +---------------------------------------------+
    1 row in set (0.001 sec)
    */

    update json_sample
    set
      item = json_remove(item, "$.double_num")
    where
      id = 3
    ;
    -- Query OK, 1 row affected (0.003 sec)
    -- Rows matched: 1  Changed: 1  Warnings: 0

    select * from json_sample;
    /*
    +------+------------------------------------------------------------+
    | id   | item                                                       |
    +------+------------------------------------------------------------+
    |    1 | {"fruit": ["apple", "peach"], "animal": ["neko", "inu"]}   |
    |    2 | {"drink": ["tea", "samovar", "Matcha"], "action": "sleep"} |
    |    3 | {"num": "[1,2,3,4]"}                                       |
    +------+------------------------------------------------------------+
    3 rows in set (0.001 sec)
    */
    ```

    ### 置換する

    ```sql
    /*
    json_insertは挿入のみ
    json_replaceは更新のみ
    json_setならどっちもOKという特徴がある
    */
    update json_sample
    set
      item = json_replace(item, "$.action", "walk")
    where
      id = 2
    ;

    select * from json_sample;
    /*
    +------+-----------------------------------------------------------+
    | id   | item                                                      |
    +------+-----------------------------------------------------------+
    |    1 | {"fruit": ["apple", "peach"], "animal": ["neko", "inu"]}  |
    |    2 | {"drink": ["tea", "samovar", "Matcha"], "action": "walk"} |
    |    3 | {"num": "[1,2,3,4]"}                                      |
    +------+-----------------------------------------------------------+
    3 rows in set (0.000 sec)
    */
    ```

    ### 配列にデータ追加

    ```sql
    update json_sample
      set item = 
        json_array_append(item, "$.drink", "water")
    where
      id = 2
    ;
    /*
    +------+--------------------------------------------------------------------+
    | id   | item                                                               |
    +------+--------------------------------------------------------------------+
    |    1 | {"fruit": ["apple", "peach"], "animal": ["neko", "inu"]}           |
    |    2 | {"drink": ["tea", "samovar", "Matcha", "water"], "action": "walk"} |
    |    3 | {"num": "[1,2,3,4]"}                                               |
    +------+--------------------------------------------------------------------+
    3 rows in set (0.000 sec)
    */
    ```

    ### 検索と配列からのデータ削除

    ```sql
    --　検索
    select json_search(item, "one", "apple") 
    from json_sample;

    /*
    +-----------------------------------+
    | json_search(item, "one", "apple") |
    +-----------------------------------+
    | "$.fruit[0]"                      |
    | NULL                              |
    | NULL                              |
    +-----------------------------------+
    3 rows in set (0.000 sec)
    */

    -- json_unquote -> quoteを取り除く
    -- whereを指定しないと他の行がnullになるので注意

    set @value = "apple";

    update json_sample
    set item = 
      json_remove(
        item,
        json_unquote(json_search(item, "one", @value))
      )
    where id = 1
    ;

    select * from json_sample;
    /*
    +------+--------------------------------------------------------------------+
    | id   | item                                                               |
    +------+--------------------------------------------------------------------+
    |    1 | {"fruit": ["peach"], "animal": ["neko", "inu"]}                    |
    |    2 | {"drink": ["tea", "samovar", "Matcha", "water"], "action": "walk"} |
    |    3 | {"num": "[1,2,3,4]"}                                               |
    +------+--------------------------------------------------------------------+
    3 rows in set (0.001 sec)
    */
    ```


## Q164 JSONをmessagePackに変換する方法について知っていますか?

??? success
    ### JSONからMessagePackへの変換

    ```json
    {
      "a": null,
      "b": 10,
      "c" : [20],
      "d" : "30"
    }
    ```

    ```py
    # pip install msgpack-python
    import json
    import msgpack

    json_file_path = "./json_file.json"
    msg_file_path = "./output.msg"

    #----書き込み処理-------
    with open(json_file_path, "r") as json_file:
      data = json.load(json_file)
    
    packed_data = msgpack.packb(data)

    with open(msg_file_path, "wb") as msg_file:
      msg_file.write(packed_data)
    
    #----読み込んで使う-----
    for msg in msgpack.Unpacker(open(msg_file_path, "rb")):
      print(msg)
    
    # {b'a': None, b'b': 10, b'c': [20], b'd': b'30'}
    ```

    ```bash
    # 変換後の中身を見てみる
    # od -hやoptionなしのhexdumpだと、2byteずつ読み込み
    # 順序を逆転させるため、1byteずつ読み込ませる
    # hexdump -C等でももちろんいい

    cat output.msg | xxd
    # 00000000: 84a1 61c0 a162 0aa1 6391 14a1 64a2 3330

    #----容量が削減できていることも確認可能------

    ls -l | \
    awk '{print $5, "byte : ", $9 }'

    # 66 byte :  json_file.json
    # 16 byte :  output.msg
    ```

## Q165 デバイス、受信サーバ間の通信方式について知っていますか?

??? success
    ### 通常のデータの場合

    ```text
    ・社内DBにあるデータや、パブリックデータ
      ⇒既に集まっている

    ・WebサイトログやSNSデータ
      ⇒Webサイトログなら、ログを
        SNSデータなら、提供しているAPIを
        許可されているなら、スクレイピングを使う
    ```

    ### IoT受信サーバでデバイスと通信する場合

    ```text
    ・通信の方法は以下の様になる
    
    HTTPプロトコル
      ・通常のWebシステムと同様に、HTTPプロトコルを利用した
      　Web APIを用いてデバイスからアクセス
      ・たとえば、GET/POST
    
    WebSocket
      ・音声や動画のリアルタイム通信を行う
      ・HTTPプロトコルと比べ、接続を継続したままに
        しておけるという利点がある
    
    MQTT
      ・第三の媒体を介して、キュー方式を利用する
      ・1つのパブリッシャーから多数のサブスクライバーへの
        通信が可能
      ・サービス品質を選ぶことができる
      ・受信サーバ側がダウンしていても、
        情報はキューに残り続けるので、データの損失が防げる
    ```

## Q166 処理サーバでのデータ加工方法と、加工を行わない場合もあることを知っていますか?

??? success
    ### 前提

    ```text
    ・蓄積サーバにデータを格納する前に、
      処理サーバでデータの加工が行われることがある
    ```

    ### 処理サーバでの処理方法

    ```text
    ストリーム処理
      ・処理サーバに到着したデータを逐次処理する方法
      ・リアルタイムで処理し、デバイス制御に使いたい時に使用
    
    バッチ処理
      ・処理とデバイス制御にタイムラグがあっても
        問題ないのならこちらを使用
    
    Spark Streaming
      ・ストリーム処理を行うためのSparkのライブラリ
      ・時系列的に連続したデータ列をRDDで分割し、
        分割データに対して、小さなバッチ処理を行う

    RDD
      ・resilident distributed datasetの略
      ・分散環境でのデータの分割、並行処理、耐障害性を提供
      ・インメモリ処理が可能なので高速

    ```

    ### クレンジング（データ加工）について

    ```text
    行うこと
      ・型の統一
      ・不適合な書式や表記ゆれを修正
      ・入力ミスや明らかな誤作動、データの無記入などの排除
      ・匿名化処理(摂動法と暗号法に分かれる)
    ```

    ### ETLとELT

    ```text
    ETL
      ・Extract Transform Load
      ・抽出してから加工し、蓄積先に書き込む
    
    ⇒上記の例はこっち

    ELT
      ・Extract Load Transform
      ・抽出したら書き込み、利用時に加工する
    
    ⇒処理サーバで加工は行われない
    ```

## Q167 DWH, Data-mart, Data-Lakeについて知っていますか?

??? success
    ### DWH(Data Ware House)

    ```text
    ・基幹システムなどから必要なデータを収取し、
      時系列上に蓄積したもの（様々なトピックが格納される）
    
    ・長期保存用のRDBであり、データは主に蓄積される
      通常のDBの様に頻繁に削除されない

    ・通常のDBと比べ、高いデータ分析性能を持つ
    ```

    ### Data-Lake

    ```text
    ・データの巨大なプール
    ・DWHと比べ、非構造化データも格納可能という特徴を持つ
    ・保存する際には加工は行われず、
    　取り出す際に、ELTが行われる
    ```

    !!! warning
        ### 匿名化処理について

        ```text
        ・格納前に加工はしないといっても個人情報は別

        ・構造などは変えないままマスキングを行い、
          匿名化処理を行ったうえでデータレイクに放り込む
          可能性もある。（特に識別子については）
        
        ・https://www.linkedin.com/advice/3/how-can-you-mask-data-lake-skills-data-management-xec6f
        ```

    ### Data-Mart

    ```text
    ・特定の目的に合わせて集計したデータを保管するDB

    ・DWHから必要なものだけを抽出したDB/Tableや、
      Data-Lakeなどからデータ加工を行って抽出したもの
      もこれに当たる
    ```

    ### それぞれの使い道

    ```text
    [DWHを用いる場合]
      複数の基幹システム(構造データ&準構造データ)
      ⇒ データ加工
      ⇒ DWH
      ⇒ データ加工
      ⇒ Data-Mart

    [Data-Lakeを用いる場合]
      センサやログデータ、JSONやXML, 業務データ,画像,映像
      ⇒Data-Lake
      ⇒データ加工
      ⇒Data-Mart
    ```

## Q168 ビッグデータの処理性能を測る指標について知っていますか?

??? success
    ### スループット

    ```text
    ・一定時間で処理できるデータの総量
    ・DWHやData-lake等でデータの量が多い処理を行う時に重視
    ```

    ### レイテンシ

    ```text
    ・データ処理が終わるまでの待機時間
    ・アドホック分析、データマートや、BIツールなどで重視
    ・レスポンスが遅いと、ユーザはイライラを募らせるので
    ```

    ### 補足(用語)

    ```text
    トレードオフ
      ・多くのリクエストやタスクの同時処理を行うと、
        個々のリクエストやタスクの処理時間が長くなる
      ・個々のリクエストを最適化しようとすれば、
        同時に処理できる件数は少なくなる
    
    アドホック分析
      ・必要に応じてリアルタイムで行う分析手法
    
    BIツール
      ・Business Intelligence
      ・経営で得たデータなどから分析などを行えるツール
    ```

## Q169 NoSQL登場前の従来のデータベースについて知っていますか?

??? success
    ### 行指向DB（さんざんやってきたもの）

    ```text
    ・行単位でデータを抽出
    ・ディスクI/Oの軽減は難しい
    ・トランザクション制御が可能
    ```

    ### 列指向DB

    ```text
    ・テーブルのデータを列単位にまとめて保存する
    ・必要な項目（列）だけを読み込めるので、ディスクI/Oの
      軽減が可能
    ・RDBの枠を出ることはない(事前のスキーマ定義は必要)

    [デメリット]
      ・データベースが列ごとに圧縮されているので、
        更新/削除時に、圧縮、展開動作が頻発する
        よって、これらを含んだトランザクション制御は苦手
    ```

    ### MPPデータベース

    ```text
    ・Massively Parallel Processing
    ・1つのクエリを小さなタスクに分解することで、
      複数のCPUコアを並列稼働して、時間を短縮する手法
    
    ・つまり、スケールアウトが可能

    ・一般的には大規模な構造化データに対して使用する
    ⇒じゃあ、非構造化データは?
    ```

## Q170 ACIDとBASE,それからCAPについて知っていますか?

??? success
    ### ACID

    ```text
    ・従来よりあるトランザクションの考え方

    A(原子性：Atomicity)
      ・トランザクションに含まれるタスクが
        すべて実行されるか、すべて実行されないか。
        そのいずれかになると保証する性質

      ・トランザクションを最小単位に見立て、これ以上
        分割できないとしている
    
    C(整合性:Consistency)
      ・トランザクション中は予め与えられた整合性を
        常に満たし続ける
      ・整合性条件に違反したトランザクションは
        実行が中断される
      
      ・Check属性やunsignedに違反するような
        送金が却下されるのもこの影響
    
    I(独立性:Isolation)
      ・トランザクション中に行われる操作は
        他の操作や他のトランザクションから隠蔽される
    
    D(永続性:Durability)
      ・Commitを行ったらその操作は永続的であり失われない
      ・Commitを行った瞬間、つまり永続Storageに反映される  
        前に電源が落ちた場合も復旧できるようにしておく
    ```

    ### ACIDの問題点

    ```text
    性能の劣化
      ・ACIDを厳密に実装しようとすると、
        広範囲なロックや大量のデータの複製等が必要になる
      ⇒これは性能面の劣化を意味する

    独立性の難しさ
      ・複数のトランザクションを並列実行する際に、
        完全な独立性を実現させるのはコストが高い
      ・そのため現実的にはトランザクション分離レベルを
        設定することになる(後述)
    ```

    ### BASE

    ```text
    ・Basically Available Soft-state Eventual consistency

    
    採用される箇所
      ・分散されたシステムで可用性や処理速度を重視したい場合
    
    Basically Available
      ・障害や分散による遅延が発生しても
        システムは基本的に利用可能
    
    Soft State
      ・一時的な状態の不整合を許容する
    
    Eventually Consistent
      ・最終的には整合性が取れている状態になる
        (時間の経過とともにデータの整合性が回復する)
    ```

    ### CAP（可用性と整合性はトレードオフなのか）

    ```text
    CAP定理
      ・分散されたシステムでは、以下のうち2つまでしか
        同時に保証できない
        Consistency（一貫性）
        Availability（可用性）
        Partition tolerance（ネットワークの分断耐性）

      ・分断体制は分散システムにおいて基本的に必須なので
        AとCのうち、どちらかをあきらめることになる
    
      ・一貫性
        ・どのノードに接続しても、すべてが同じデータを返す
    
      ・可用性
        ・1つ以上のノードがダウンしていても、
          別のクライアントが代わりに有効な応答を返す

      ・Partition tolerance
        ・分散システム内の2つのノード間の通信が
          一時的に遅延したり、分断されても
          システムが動作し続けること
    ```

    !!! warning
        ### CAP定理の問題点

        ```text
        ・単純すぎる
        ・多くのシステムはCPとAPに完全に分類する事はできない
        
        ・可用性、一貫性、分割体制については段階的な
          レベルが存在し、2つまで保証すべきというより
          トレードオフというのが正しいような気もする
        
        ・つまり、現実的に考えると、
          ネットワークが分断されないときは
          一貫性も可用性も大体満たされている
        
        ・分断された時、一貫性と可用性の優先度合いを
          幾つかの段階的なレベルで考えなければならない
        
        ※そもそもネットワークの障害が起きない自信があるなら
        　Pは考えなくてもいいわけで、ACが満たされるだろう
        ```

    ### CPデータベース

    ```text
    ・可用性を犠牲にして、一貫性と分断耐性を提供

    ・任意の2つのノード間でネットワーク分断が起きると、
      システムはこれが解決されるまで、一貫性のないノードを
      使用不可にする（つまり可用性はダメ）
    
    ・ACIDの考え方を用いる場合、こちらに近くなる
    ```

    ### APデータベース

    ```text
    ・一貫性を犠牲にして、可用性と分断体制を提供

    ・ネットワーク分断が起きても、どのノードも利用可能だが
      分断が起きている箇所は、他のノードより古いversionの
      データを返す可能性がある
    
    ・分断が解決されれば、システムはノードを再同期して、
      全ての不整合を修復する
    
    ・BASEの考え方を用いる場合、こちらに近くなる
    ```

## Q171 トランザクション分離レベルについて知っていますか?

??? success
    ### 分離レベルによって起こりうる問題?

    ```text
    ※2つのトランザクションが並列で行われているとする
    ※またそれぞれの操作は被らないものとする

    Dirty Read
      ・トランザクションAでデータ取得
      ・トランザクションBでデータ変更
      ・トランザクションAでデータ取得　⇒変わっている
      ・トランザクションBをrollback
    
    Fuzzy Read
      ・トランザクションAでデータ取得
      ・トランザクションBでデータUpdate & Commit
      ・トランザクションAでデータ取得　⇒内容が変わっている
    
    Phantom Read
      ・トランザクションAでデータ取得
      ・トランザクションBでデータ(Insert / Delete) & Commit
      ・トランザクションAでデータ取得　⇒　行数が変わっている
  
    ```

    ### 分離レベルについて

    ```text
    Read Uncommitted
      ・Dirty Read/Fuzzy Read/Phantom Readが発生する
      ・通常はロックを待機するような状態で、待機せず実行する
      ・結果、Commitされていないデータまで読み取るので
      　並行処理によって正確性が損なわれる可能性が高い
    
    Read Committed
      ・Fuzzy Read/Phantom Readが発生する
      ・Commitされたデータを常に読み取るので、
        別のトランザクション内でCommitされた内容を
        読み取ってしまう
      ・PostgreSQL, SQL Server, Oracleのデフォルト
    
    Repeatable Read
      ・一度でもトランザクションが読み取っていれば、
        次回の読み取りではその時のShapShotが提供される
      ・よって、Fuzzy Readは起こらない
      ・しかし、Phantom Readは起こる
      ・MySQLのデフォルト
    
    Serializable
      ・読み取るすべての行に共有ロックをかける
      ・つまり、同一トランザクション中に
        他のトランザクションの影響を受けることはない
       ⇒待機時間が増えるので実用的ではない
    ```

    ### MariaDBで実際に試してみる(Uncommitted/Committed)

    ```sql
    create table player_data (
      id int primary key auto_increment,
      coin int
    );

    insert into player_data values
    (1, 100),
    (2, 100)
    ;

    -- クライアントA, Bで接続
    -- Aで分離レベルをread uncommittedに変更
    set session transaction isolation level 
    read uncommitted;

    -- Bで分離レベルをread committedに変更
    set session transaction isolation level
    read committed;

    -- Bで全体のデータを閲覧
    select * from player_data;
    /*
    +----+------+
    | id | coin |
    +----+------+
    |  1 |  100 |
    |  2 |  100 |
    +----+------+
    2 rows in set (0.001 sec)
    */

    -- 2つのトランザクションをスタート
    start transaction;

    -- Bでid = 1のデータを更新
    update player_data
    set coin = 200
    where id = 1
    ;

    -- Aでデータを閲覧(Dirty Read)
    select * from player_data;
    /*
    +----+------+
    | id | coin |
    +----+------+
    |  1 |  200 |
    |  2 |  100 |
    +----+------+
    2 rows in set (0.001 sec)
    */

    -- Aでデータを更新
    update player_data
    set coin = 300
    where id = 2
    ;

    -- Bでデータを閲覧(変わっていない)
    select * from player_data;
    /*
    +----+------+
    | id | coin |
    +----+------+
    |  1 |  200 |
    |  2 |  100 |
    +----+------+
    2 rows in set (0.007 sec)
    */

    -- Aでcommit
    commit;

    -- Bでデータを閲覧(Fuzzy Read)
    select * from player_data;
    /*
    +----+------+
    | id | coin |
    +----+------+
    |  1 |  200 |
    |  2 |  300 |
    +----+------+
    2 rows in set (0.001 sec)
    */

    
    ```

    ### MariaDBで試してみる(Repeatable Read)

    ```sql
    -- デフォルトなので変更は必要ない

    -- 2つのtransaction A,Bをスタート

    -- Bでデータを読み込んでおく
    select * from player_data
    where id = 2;
    /*
    +----+------+
    | id | coin |
    +----+------+
    |  2 |  300 |
    +----+------+
    1 row in set (0.008 sec)
    */

    -- Aでデータ更新 && commit
    update player_data
    set coin = 400
    where id = 2
    ;

    commit;

    -- Bでデータ閲覧(変わっていない)
    select * from player_data
    where id = 2;

    /*
    +----+------+
    | id | coin |
    +----+------+
    |  2 |  300 |
    +----+------+
    1 row in set (0.001 sec)
    */

    -- Bでcommitした後で見ると更新されている
    commit;
    select * from player_data;
    /*
    +----+------+
    | id | coin |
    +----+------+
    |  1 |  200 |
    |  2 |  400 |
    +----+------+
    2 rows in set (0.000 sec)
    */

    -- Phantom Readに関して
    create table player_data2 like player_data;

    -- 2つのトランザクションA,Bを開始
    start transaction;

    -- Bで閲覧しておく
    select * from player_data2;
    -- Empty set (0.000 sec)

    -- Aで挿入 && commit
    insert into player_data2 values
    (1, 100)
    ;
    commit;

    -- Bで閲覧 PhantomReadも起こっていない
    -- これはInnoDB特有の挙動でMVCCという技術に由来している
    select * from player_data2;
    -- Empty set (0.000 sec)
    ``` 

    ### [補足]予め読み取っていなかった時の挙動

    ```sql
    -- 予め読み取っていない場合の挙動
    -- 読み取っていない(=一度も該当行がスキャンされていない)

    --　現在の中身
    /*
    +----+------+
    | id | coin |
    +----+------+
    |  1 |  100 |
    +----+------+
    1 row in set (0.001 sec)
    */

    -- 2つのトランザクションをスタート
    start transaction;

    -- Aで変更し、commit
    update player_data2
    set coin = 200
    where id = 1;
    commit;

    -- Bで閲覧(予め読んでいなかったので更新されている)
    select * from player_data2;
    /*
    +----+------+
    | id | coin |
    +----+------+
    |  1 |  200 |
    +----+------+
    1 row in set (0.001 sec)
    */
    ```

    ### MariaDBで試してみる(Serializable)

    ```sql
    -- トランザクション2つをスタートさせる
    begin;

    -- トランザクションAの分離レベルをserializableに
    set session transaction isolation level
    serializable;

    -- Aで検索しておく
    select * from player_data2;
    /*
    +----+------+
    | id | coin |
    +----+------+
    |  1 |  200 |
    +----+------+
    1 row in set (0.000 sec)
    */

    -- Bで追加しようとする
    insert into player_data2 values
    (2, 400);

    -- 止まった
    -- Aが読み取ったすべての行に共有ロックをかけているため
    -- ERROR 1317 (70100): Query execution was interrupted
    
    -- Aのトランザクションを終了
    commit;

    --　再度追加(終了したので可能になる)
    insert into player_data2 values
    (2, 400);
    ```

## Q172 MVCCについて知っていますか?

??? success
    ### MVCC基礎

    ```text
    ・Multi-Version Concurrency Control

    ・InnoDBは、各行に、非表示の3つの列を追加する

      DB_ROW_ID
        ・unique or primary keyがない場合に追加される

      DB_TRX_ID
        ・行を挿入/更新した最後のtransaction識別子が入る
      
      DB_ROLL_PTR
        ・undoログを辿るためのポインタ
        ・行が更新された場合、undoログレコードに
          更新前の内容を再構築するための情報が書かれる

    ・InnoDBは、insert/update/deleteをundoログに記録する
      ただし、deleteは内部的にはupdateとして記録される
      これはInnoDBが行を物理的に削除するのではなく、
      削除フラグを更新しているためである
    
    ・論理的な削除フラグを用いることで、
      行の前のバージョンをundoログを用いて取得可能になる
      ※行が削除されている場合では不可能
    ```

    ### トランザクションID
    ```text
    ・新しいトランザクションが開始された時に取得される
    ・start transactionやbeginが実行されるとき
    ```

    ### 最新バージョンを読み取る場合

    ```text
    ・select * ... lock in share mode(共有ロック)
    ・select * ... for update (排他ロック)
    ・insert/update/delete操作

    ⇒後述のReadViewは生成されない
    ```

    ### snapshotを介して必要なバージョンを読み取る場合

    ```text
    ・ロックのないselect
    ⇒後述のReadViewを生成
    ```

    ### undoログ

    ```text
    挿入取り消しログ
      ・insertで生成
      ・rollback時に使用される
      ・commitされたら破棄
    
    更新取り消しログ
      ・update/deleteで生成
      ・rollback時に使用
      ・snapshotの読み取りにも使用
      ・commit後、logレコードがDBで使用されているsnapshotに
        含まれていない場合は、破棄される

    ```

    ### バージョンチェーンについて

    ```text
    複数のトランザクションが同一レコードに対して
    同時に動作する場合を考える

    1 たとえば、トランザクションID1によって
      name = test2と更新されていたデータがあるとする

    2 ここで、トランザクションID2がname = test1と更新する。
      すると、現在のデータは
      name = test1, db_trx_id = 2となる
      また、db_roll_pointerは以前のデータを指し示す
    
    3  name = test2, db_trx_id = 1のデータはundoログに移動
       2つのバージョン間はdb_roll_pointerで参照可能

    4 トランザクションID3でname = testと更新する
      現在のデータはname = test, db_trx_id = 3
    
    5 name = test1, db_trx_id = 2のデータはundoログに移動

    | version   | name  | db_trx_id | db_roll_pointer |
    | --------- | ----- | --------- | --------------- |
    | latest    | test  | 3         | undo log2を示す |
    | undo log2 | test1 | 2         | undo log1を示す |
    | undo log1 | test2 | 1         | 0x0001          |
    ```

    ### ReadViewについて

    ```text
    ・TransactionがSnapshotを介して読み取りを行う時に
      生成される(つまりロックを行わないselect)
    
    ・該当行が、現在のトランザクションで表示されるか否かを
      チェックするために使用
    
    ReadViewは以下のものを保存する

      trx_ids:
        アクティブなトランザクションID
        自分自身と、既にcommitしたものは含まない
      
      low_limit_id
        ・割り当てられる次のトランザクションID
        ・トランザクションIDはincrementなので

      up_limit_id
        ・trx_idsの最小トランザクションID

      creator_trx_id
        ・ReadViewを生成したトランザクションID
        ・自身で変更したデータならアクセスしたいので必要
    
    [ルール]
      アクセスversion = Creator_trx_idなら
        ・該当行は現在のトランザクションで変更されているので
          そのverにアクセスして終了
    
      アクセスversion < up_limit_idなら
        ・このversionを生成したトランザクションは
          ReadViewを生成する前にcommitされている
        ・よって、現行のトランザクションからアクセス可能
      
      アクセスversion > low_limit_id
        ・このversionを生成したトランザクションは
          ReadViewを生成後に、開かれている
        ・よってアクセス不可能
      
      up_limit_id < アクセスver < low_limit_id
        ・アクセスversionを生成したトランザクションIDが
        　trx_idsにあるか確認
        ・ある場合、未commitなのでアクセス不可能
        ・ない場合は、commit済みなので、アクセス可能
      
    ```

    ### 実際の流れ

    ```text
    1 トランザクション開始。trx_idを取得

    2 Select文が実行されるとReadViewを取得

    3 データが見つかったら、ReadViewのトランザクションIDと
      比較を行う

    4 アクセス不可ならundo logを使って、バージョンを遡る

    5 アクセス可なら該当行の情報を表示する

    ⇒上記の様にしてPhantom Readを防いでいる
    ```

## Q173 NoSQLについて知っていますか?

??? success
    ### 由来

    ```text
    ・Not only SQL
    ・RDBMS以外のDBの総称を意味する
    ```

    ### RDBMSと比較する

    | -              | RDBMS          | NoSQL          |
    | -------------- | -------------- | -------------- |
    | 適したデータ   | 構造化データ   | 非構造化データ |
    | スキーマ定義   | 事前定義必要   | 事前定義不要   |
    | スキーマ変更   | 変更しにくい   | 変更しやすい   |
    | 整合性         | ACID           | BASE/ACID      |
    | データ操作命令 | S/I/U/D        | Put, Get       |
    | 拡張方式       | スケールアップ | スケールアウト |


    ### NoSQL補足

    ```text
    ・構造定義が不要なので、非構造化データが向いている

    ・トランザクション制御には不向き（不可能ではない）

    ・他データとの関係がないためJOINがない

    ・複雑な結合等が行われないため、
      スケールアウトが比較的容易。反面、整合性は弱い
    ```

    !!! info

        ### たとえばDynamoDBは

        ```text
        ・NoSQLだがACID特性と、トランザクションを有する
        ・必ずしもRDBMSならACID, NoSQLならBASEというわけで
          はないことに注意。上記の表はあくまでも傾向
        ```
    
    ### スケールアップ vs スケールアウト

    ```text
    スケールアップ
      ・ハードウェアの性能を向上させ、
        特定の1台のサーバ性能を増強する

      ・厳密な整合性を維持しながら複数サーバを用いるのは
        難しいため（そもそも手動シャーディングが難しいと
        いう側面もある）RDBMSの場合、大体こちらが採用される
    
    スケールアウト
      ・サーバの台数を増やし、全体での処理能力を向上させる

      ・高性能なサーバを用いる必要がないので、低コスト

      ・サーバが1台停止しても、全システムが停止する
        わけではないので可用性が高くなる

      ・JOINや複雑なクエリを採用しないNoSQLは複数サーバへの
      　分割、複製が可能であり、この手法が用いられる
    ```

    ### ビッグデータにはどちらが向いているか

    ```text
    NoSQL
      ・スケールアップには限界がある
      ・音声/映像/画像データなどはRDBMSでは扱えない
    ```

## Q174 NoSQLの種類について知っていますか?

??? success
    ### Key-Value型

    ```text
    概要
      ・keyとvalueの組でデータを格納する
      ・keyはvalueの識別子
      ・key/valueいずれにも複雑なオブジェクトを指定できる

    非構造化データについて
      ・binaryデータであれば、blob型の画像や音声も格納可能

    スケールアウト
      ・サーバ毎に保存するデータのキーの範囲を設定
      ・なお、障害に備え、1つのキーは複数のサーバで
        冗長的に管理されることが一般的
    
    採用例
      ・DynamoDB

    ```

    ### カラム指向型

    ```text
    概要
      ・key-value型の強化版
      ・keyが複数のカラムを持つことが可能
      ・I/Oを考え列単位でデータの保存や読み込みも可能
      ・一部の列のデータが欠けていても問題ない
      ・行列の数は拡張可能
    
    採用例
      ・Cassandra
    ```

    ### ドキュメント型

    ```text
    概要
      ・JSONやXML等のデータ形式のドキュメントを格納する
      ・各ドキュメントには識別IDが振られる
      ・スキーマは必要ない
    
    採用例
      ・MongoDB
    ```

    ### グラフ型

    ```text
    概要
      ・グラフによって、データ同士の関係を表現する
      ・スケールアウトはシャーディングなしには厳しい

    ノード
      ・データオブジェクトを格納するグラフの頂点

    エッジ
      ・ノード間の関係性を示したもの
    
    プロパティ
      ・ノードとエッジの具体的な属性を示したもの
    
    使用ケース
      ・SNSのフォロー、フォロワー関係
      ・不正検出
    
    採用例
      ・Neo4j
    ```

## Q175 NoSQLの構成について知っていますか?

??? success
    ### マスタ型

    ```text
    ・メタ情報はマスタノードで管理する
    ・つまり、マスターノードが単一障害点(SPOF)になる

    ⇒整合性重視
    ```

    ### P2P型

    ```text
    ・クラスタを構成するノードはすべて等価
    ・単一障害点はない
    ・ネットワーク分断やノードの障害が発生した場合、
      データの一貫性が保たれなくなる可能性あり

    ⇒可用性重視
    ⇒ただし、レプリケートによって整合性方面の
      弱点は軽減可能

    ⇒たとえばInfiniteGraphはp2p型であるにもかかわらず
      ACID特性を保証している
    ```

    ### オンメモリ型

    ```text
    ・メモリ上で動作するのでデータは揮発する
    ・メモリ上なので、高速処理が可能

    ⇒永続性については他のDBと組み合わせる
    ```

    ### オンディスク型

    ```text
    ・複数のサーバにデータベースを分割して保有不可
    ```

    ### アーキテクチャを含んだ分類表

    | -         | マスタ  | P2P           | オンメモリ | オンディスク |
    | --------- | ------- | ------------- | ---------- | ------------ |
    | key-value | Hibari  | Dynamo        | Redis      | Tyrant       |
    | column    | Hbase   | Cassandra     | -          | -            |
    | graph     | Neo4j   | InfiniteGraph | -          | -            |
    | document  | MongoDB | -             | -          | -            |

## Q176 NoSQLの整合性を実現する技術について知っていますか?

??? success

    ### 整合性とレイテンシのトレードオフ

    ```text
    ・書き込んだノード数をW, 
      読み出したノード数をR, 
      全体のノード数をNとする

    ・この時、W + R > Nであれば、
      読み出し時に、書き込まれたノードに必ず到達できるため
      整合性を保証できる

    ・ただし、書き込み数と読み出し数を増やすほど
      レイテンシが大きくなる
    ```

    ### Version管理

    ```text
    DynamoDBを例に出して考える
      ・P2Pなので、あるノードで起こった処理を
        システム全体にマルチキャストする必要がある
      ・最新のイベントを特定する方法はあるだろうか？
      
    TimeStamp
      ・データの更新時間を記録する
      ・新しいTimeStampを持つものを重要視する
    
    TimeStampの問題点
      ・物理的な絶対時間はある程度の誤差が出る
      ⇒最新かどうか保証できない

    Vector Clocks
      ・ノード数をnとしたとき、n次元のvectorを持たせる
      ・各ノードの成分は、ノードが現在知っている
        各ノードで起こったイベントの数となる
    
    更新ルール
      ・自ノードでイベントが発生したら、
        成分のうち、自分を示す箇所をincrement

      ・自ノードから、変更したという旨を他のプロセスへ
        送信する場合は、自身のn次元ベクトルを添付する
        添付した後は、自分を示す箇所をincrement
      
      ・受信した時は、自身を示す箇所をincrement
        そのあとで、添付されていたn次元ベクトルと比較して、
        大きい方を残す
    
    例)

      A[0,0,0]
      B[0,0,0]
      C[0,0,0]
    
      プロセスA,Bでイベント発生
      A[1,0,0]
      B[0,1,0]
      C[0,0,0]

      プロセスA -> Bにメッセージ送信
      (送信, 受信でincrementしてから比較している)
      A[2,0,0]
      B[2,2,0]
      C[0,0,0]

      プロセスC -> Bにメッセージ送信
      A[2,0,0]
      B[2,3,1]
      C[0,0,1]

      プロセスA, Cでイベント発生
      A[3,0,0]
      B[2,3,1]
      C[0,0,2]

      プロセスB-> Aにメッセージ送信
      A[4,4,1]
      B[2,4,1]
      C[0,0,2]   

      プロセスA,Bでイベント発生
      A[5,4,1]
      B[2,5,1]
      C[0,0,2]

      ここで順序を比較する
      ∀i v1[i] <= v2[i]
      ∃i v1[i] < v2[i]
      なら、v1はv2より前に起こったといえる

      たとえば、A[4,4,1]とC[0,0,2]の場合、
      どちらが前かは断言できない

      A[4,4,1]と、B[2,3,1]なら
      A[4,4,1]の方が新しいと断言できる

    Vector Clocksの問題点
    ```